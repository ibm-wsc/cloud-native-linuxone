{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Cloud Native LinuxONE Workshop","text":"<p>Welcome to our Cloud Native LinuxONE workshop. Developers can leverage OpenShift to create continuous integration pipelines for Linux\u00ae workloads on IBM Z\u00ae and LinuxONE. You can quickly get up and running on OpenShift with a continuous workflow.</p>"},{"location":"#agenda","title":"Agenda","text":""},{"location":"#introductory-presentations","title":"Introductory Presentations","text":"<ul> <li>12:00 - 12:30pm ET - Introduction to Kubernetes and OpenShift</li> <li>12:30pm - 1:00pm - Introduction to Cloud Native DevSecOps</li> </ul>"},{"location":"#lab-build-and-deploy-a-cloud-native-devops-pipeline-in-openshift-on-ibm-z-and-linuxone","title":"Lab: Build and Deploy a Cloud Native DevOps Pipeline in OpenShift on IBM Z and LinuxONE","text":"<ul> <li>1:00pm - 1:15pm - Introduction to Cloud Native DevSecOps Pipeline Lab</li> <li>1:15pm - 5:00 pm<ul> <li>Deploy PetClinic via OpenShift Pipelines</li> <li>Configure PetClinic's Integration and Deployment via OpenShift Pipelines to Meet Your Organization's Needs1</li> <li>Extend Pipeline to Upgrade from Development to Staging</li> <li>Configure SonarQube code analysis in your Pipeline</li> </ul> </li> </ul>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<ul> <li> <p>Thanks to the following contributors:</p> <ul> <li> <p>Chee Yee for setting up the LinuxONE Community Cloud for the OpenShift Pipelines workflow</p> </li> <li> <p>The <code>spring</code> developers for creating the petclinic demo and the <code>redhat-developer-demos</code> for sharing the <code>spring-petclinic</code> version of sample application we started with</p> </li> </ul> </li> </ul>"},{"location":"#key-contributors","title":"Key Contributors","text":"<ul> <li>Barry Silliman</li> <li>Faraz Khan</li> <li>Rishi Patel</li> <li>William Doyle</li> </ul>"},{"location":"#workshop-authors","title":"Workshop authors","text":"<ul> <li>Garrett Woodworth</li> <li>Jin VanStee</li> </ul> <ol> <li> <p>For the purposes of this lab, you are fulfilling the requirements of a fictional organization. These requirements could change for your specific organization but would follow a similar pattern with different specifics.\u00a0\u21a9</p> </li> </ol>"},{"location":"assignment/","title":"Lab Assignments","text":"<p>You will need to enter the \"Skytap Password\" one time only (assuming you don't switch browsers or clear cache).</p> <p>If your Red Hat Enterprise Linux desktop asks for a login password for userid workshop user, the password, for all teams, will be provided by an instructor in the WebEx meeting chat. Your desktop will lock and ask for this password if your session is idle for one hour.</p> <p>You only need to use this instance for the RBAC lab, and only if you do not have the OpenShift CLI (oc) installed on your local workstation.</p> Name Team Number Skytap URL Skytap Password Andrew 01 http://ibm.biz/zdevops01 6l4wqd6n Ashraf 02 http://ibm.biz/zdevops02 dpd5pcnl Cole 03 http://ibm.biz/zdevops03 lxs5wi4t Dale 04 http://ibm.biz/zdevops04 3m837lnf Emanuele 05 http://ibm.biz/zdevops05 strbcx0f Erwin Jesus 06 http://ibm.biz/zdevops06 1h9ouksr Feng 07 http://ibm.biz/zdevops07 a3zvy38d Frank 08 http://ibm.biz/zdevops08 0m8as1tf Geo 09 http://ibm.biz/zdevops09 94nng2q9 Jordan 10 http://ibm.biz/zdevops10 i66vtvl9 Luciana 11 http://ibm.biz/zdevops11 u86khpjm Michael C. 12 http://ibm.biz/zdevops12 3wuw3td3 Michael E. 13 http://ibm.biz/zdevops13 p9qc1j7v Neil 14 http://ibm.biz/zdevops14 dq5xop18 Peter 15 http://ibm.biz/zdevops15 tt8qh2pi Rain 16 http://ibm.biz/zdevops16 mu11cfii Rakan 17 http://ibm.biz/zdevops17 tlslymja Steve 18 http://ibm.biz/zdevops18 9lbb4twe Terry 19 http://ibm.biz/zdevops19 oaxdvc9m Tom 20 http://ibm.biz/zdevops20 kpzpphp2"},{"location":"introduction/","title":"Cloud Native Workshop Introduction","text":"<p>You will build and deploy an application (the cloud native way) using OpenShift Container Platform and its CI/CD workflow OpenShift Pipelines.</p>"},{"location":"introduction/#lab-overview","title":"Lab Overview","text":"<p>You will set up a virtual pet clinic (based on the classic spring boot demo referenced in the main documentation) running on LinuxONE using source code on GitHub and OpenShift Pipelines to seamlessly update, test, and deploy your pet clinic.</p>"},{"location":"introduction/#lab-parts","title":"Lab Parts","text":"<p>This lab is broken into three main parts:</p> <ol> <li> <p>Building and deploying the PetClinic Java application with OpenShift Pipelines</p> </li> <li> <p>Configuring PetClinic's integration and deployment pipeline to meet your organization's needs1</p> </li> <li> <p>Promoting PetClinic from development to staging with testing and GitHub integration (adding the C [continuous] in CI/CD)</p> </li> </ol>"},{"location":"introduction/#bonus-part","title":"Bonus Part","text":"<ol> <li>Securing your pipeline with SonarQube to put the Sec in DevSecOps</li> </ol>"},{"location":"introduction/#architecture-overview","title":"Architecture Overview","text":"<p>Throughout your journey across the three main parts listed above, you will slowly construct the following architecture:</p> <p></p>"},{"location":"introduction/#define-your-pipeline-flow-orange-lines-in-picture-above","title":"Define your Pipeline Flow (orange lines in picture above)","text":"<p>a. Interact with the OpenShift Pipelines UI to define your pipeline and other Tekton components.</p> <p>b. OpenShift Pipelines UI creates the resources you defined in your OpenShift project</p>"},{"location":"introduction/#run-your-pipeline-flow-purple-lines-in-picture-above","title":"Run your Pipeline Flow (purple lines in picture above)","text":"<ol> <li>git push changes to your code to to GitHub</li> <li>The GitHub webhook sends a push event to the event listener URL</li> <li>The event listener triggers a new pipeline run based on the <code>PipelineRunTemplate</code> and parameters populated from GitHub</li> <li>git clone the newly updated code from GitHub</li> <li>store this code in a persistent Tekton workspace. Use this workspace to store data and changes during pipeline runs</li> <li>make sure mysql is up to date with definition in GitHub</li> <li>build container image with newly updated code (testing it during the build) and then push the image</li> <li>update the PetClinic application with the new image tag and other dev or test parameters as well as any changes to Kubernetes definitions</li> <li>pull newest PetClinic container image to deploy code changes</li> <li>test external connection via route</li> <li>use PetClinic from your web browser to test out the updated code, including new features added.</li> </ol> <p>Note</p> <p>The pipeline completes Steps 8-10 first for the development environment. Once they complete, the pipeline will tear down that environment and repeat steps 8-10 for the staging environment, before moving on to step 11.</p> <ol> <li> <p>For the purposes of this lab, you are fulfilling the requirements of a fictional organization. These requirements could change for your specific organization but would follow a similar pattern with different specifics.\u00a0\u21a9</p> </li> </ol>"},{"location":"logistics/","title":"Lab Logistics","text":"<p>The lab will be performed against a Red Hat OpenShift 4.12 cluster running in the IBM Washington Systems Center internal network.</p> <p>Because it is in an internal network, it is accessible only through a Virtual Private Network (VPN) connection. The instructors have already logged each student enivornment into the VPN.</p> <p>During class you will be given a URL by the instructors which you will use to get a student environment to work with.  You will be asked to enter a password, which will be provided to you by the instructors.</p> <p>Then, within the student environment, you will use another password to log in to the desktop.  Your instructors will give you this password as well.</p> <p>During the lab, you will use three other sets of credentials which will be given to you by the instructors.</p> <ol> <li>A userid and password for logging in to the Red Hat OpenShift Cluster</li> <li>A userid and password for logging in to Gogs (Go-based Git Server)</li> <li>A userid and password for logging in to the SonarQube server.</li> </ol>"},{"location":"proposechange/","title":"How to propose a change","text":"<p>On the page you want to make a change to, click on the paper with pencil icon next to the page's title. This will take you to edit the page in Github.</p> <p>You will see a message similar to the following: \"You\u2019re editing a file in a project you don\u2019t have write access to. Submitting a change to this file will write it to a new branch in your fork, so you can send a pull request.\"</p> <p>Make your changes in Markdown. And submit for review.</p> <p>The owners of this repo will review your pull request and accept or deny your change proposal.</p> <p>There are other ways of doing a pull request, a Google search will lead you to those tutorials.</p>"},{"location":"resources/","title":"Other Resources","text":""},{"location":"resources/#openshift-pipelines-resources","title":"OpenShift Pipelines Resources","text":"<ul> <li>Information Page</li> <li>Intro Series on OpenShift blog</li> </ul>"},{"location":"cloud-lab/prerequisites/","title":"Prerequisites","text":""},{"location":"cloud-lab/prerequisites/#create-a-github","title":"Create a GitHub","text":"<p>Create a GitHub account (if you don't already have one) here.</p>"},{"location":"cloud-lab/prerequisites/#create-a-linuxone-community-cloud-openshift-container-platform-trial","title":"Create a LinuxONE Community Cloud OpenShift Container Platform Trial","text":"<p>Create a LinuxONE Community Cloud OpenShift Container Platform trial here. Within 48 hours after you register, be sure to \"Activate your account or entitlement\" following the instructions in the follow-on email sent to the email address you specified during registration.</p>"},{"location":"cloud-lab/yamlsetup/","title":"Quick Start PetClinic pipeline with Yaml files","text":"<p>This section is for users that want to quickly bring up the pipeline and resources of the lab from yaml. This is good for users that have gone through the lab and want to learn how to quickly bring up the resources in a different cluster. This is what users will eventually want to do with pipelines they create (use yaml files to portably bring pipelines to different OpenShift clusters).</p> <p>Warning</p> <p>If you already have resources created from the lab, please do the cleanup section here.</p>"},{"location":"cloud-lab/yamlsetup/#pre-requisites","title":"Pre-requisites","text":"<ol> <li> <p>Code for the project</p> <p>Please fork the code to your GitHub repository by clicking here. If this fails, you likely already have a forked version of the repository from the lab. Please make sure you have done the cleanup section here and then come back to this section.</p> </li> <li> <p>Clone the git repository to your local computer </p> <p>a. Get the link from GitHub using the <code>Code</code> button on your forked repository and the <code>HTTPS</code> tab. </p> <p>b. Perform the clone locally in a terminal window using git clone + the link you copied such as <code>git clone https://github.com/siler23/spring-petclinic.git</code> for the example in <code>a.</code> above.</p> </li> <li> <p>Log into OpenShift in a terminal window locally.</p> <p>a. Click on your username in the upper right hand of the LinuxONE Community Cloud OpenShift UI.</p> <p>b. Click <code>Copy Login command</code></p> <p></p> <p>c. In the new window that opens click <code>Display Token</code> to generate a login token.</p> <p>Note</p> <p>You may be prompted to enter your LinuxONE Community Cloud username and password again.</p> <p></p> <p>d. Copy the login command.</p> <p></p> <p>e. Use the login command in your terminal to login to your OpenShift project.</p> <p></p> <p>Note</p> <p>Login token has been blurred in image for security purposes.</p> </li> <li> <p>Create OpenShift resources from PetClinic git repo you cloned.</p> <p>a. Change into the directory where you cloned your petclinic repo in step 2.</p> <p>b. Create the necessary project files in OpenShift from the main directory of the cloned GitHub fork using the following command:</p> <pre><code>oc apply -f ocp-files/pipelines/final-yaml\n</code></pre> <p>Example</p> <pre><code>task.tekton.dev/connection-test created\npipeline.tekton.dev/spring-petclinic created\ntask.tekton.dev/kustomize created\npersistentvolumeclaim/petclinic-pipeline-pvc created\n</code></pre> </li> <li> <p>Go to your newly created spring-petclinic pipeline via the <code>Pipelines UI</code> and start a new pipeline.</p> <p></p> </li> <li> <p>Replace the YOUR_REPO with your git base repository (Git Hub username) and YOUR_PROJECT with the your OpenShift Project (the one you have in community cloud or otherwise) and choose to back the workspace with a PVC (namely the petlcinic-pipeline-pvc) you created in step 4. </p> <p>a. Initial Parameters</p> <p></p> <p>b. Change Parameters and start pipeline</p> <p>Note</p> <p>Your project and repo should be different from mine.</p> <p></p> </li> <li> <p>Your pipeline is back in action! </p> </li> </ol>"},{"location":"cloud-lab/application-promotion/","title":"It's Time to Get your Pet Clinic Ready for its Internal Debut","text":"<p>In this section, you will bring PetClinic from development to staging for the internal showcase of your pet clinic (staging).</p> <ol> <li> <p>Promotion Tasks</p> <ul> <li>Check successful connection of PetClinic <code>dev</code> version</li> <li>Deploy PetClinic <code>staging</code> version</li> <li>Check successful connection of PetClinic <code>staging</code> version</li> </ul> </li> <li> <p>Git Tasks</p> <ul> <li>Add GitHub trigger to pipeline</li> <li>Pass git commit messages and hashes to pipeline</li> <li>Version images by git commit</li> <li>Add webhook to GitHub so it will trigger a new <code>PipelineRun</code> for each push</li> </ul> </li> <li> <p>Running Pipeline</p> <ul> <li>Update PetClinic with new animal type and push to GitHub</li> <li>Watch GitHub trigger <code>PipelineRun</code></li> <li>Watch app move from dev to staging seamlessly with images tagged with git commit SHA </li> <li>Interact with the PetClinic application in staging using the new animal type</li> </ul> </li> </ol>"},{"location":"cloud-lab/application-promotion/action/","title":"CI/CD in Action","text":""},{"location":"cloud-lab/application-promotion/action/#make-a-change-in-github","title":"Make a change in GitHub","text":"<ol> <li> <p>Navigate to your GitHub fork's main page (this is the <code>Code</code> tab if you are on a different tab such as settings)</p> </li> <li> <p>Choose to <code>Go to file</code></p> <p></p> </li> <li> <p>Type (or paste) the name of the following file into the search bar and select it (copy and paste box below image):</p> <p></p> <pre><code>src/main/resources/db/mysql/data.sql\n</code></pre> <p>If the file doesn't appear right away</p> <p>Depending on your internet connection to GitHub, there may be a slight delay before the file appears for editing (your screen matches the picture above and you can click to select the file).</p> </li> <li> <p>Select to edit the file</p> <p></p> </li> <li> <p>Change the file to add the pet field of your choice and commit it to your GitHub fork (description and copy and paste box are below image)</p> <p></p> <ol> <li> <p>Make a change using the pet type you want to add (example is a turtle) </p> <p>Note</p> <p>The copy and paste box below can be entered on line 24 with <code>enter</code> pressed after it to match the image above.</p> <p>Turtle</p> <pre><code>INSERT IGNORE INTO types VALUES (7, 'turtle');\n</code></pre> <p>I want to add Willow, an awesome armadillo, not Yertle the turtle!</p> <p>If you want to add something other than a turtle as an option, please change <code>turtle</code> to that animal (i.e. <code>armadillo</code>) in the mysql statement above. For the armadillo example, the statement becomes:</p> <pre><code>INSERT IGNORE INTO types VALUES (7, 'armadillo');\n</code></pre> </li> <li> <p>Type in a commit message (you can make this whatever you want) and commit the change (example copy and paste boxes from image above)</p> <p>Yertle the turtle</p> Title<pre><code>Turtle Time\n</code></pre> Extended Description<pre><code>I want to be able to add Yertle the turtle.\n</code></pre> </li> </ol> </li> <li> <p>Take note of the git commit message and hash</p> <p></p> </li> </ol>"},{"location":"cloud-lab/application-promotion/action/#continuous-integration-via-openshift-pipelines","title":"Continuous Integration via OpenShift Pipelines","text":""},{"location":"cloud-lab/application-promotion/action/#successfully-run-pipeline-via-github","title":"Successfully Run Pipeline via GitHub","text":"<ol> <li> <p>Visit the newly triggered pipeline run in the <code>Pipelines</code> menu in the OpenShift UI</p> <p></p> </li> <li> <p>View the pipeline run from the <code>Details</code> view</p> <p></p> <p>You can see the event listener has triggered the <code>PipelineRun</code> instead of a user this time.</p> </li> <li> <p>You can see the variables populated with the correct values from Github in the <code>YAML</code> view of the pipeline run.</p> <p></p> </li> <li> <p>Watch the results of your build pipeline run. It should complete successfully as in the pictures below. </p> <p>Your pipeline may take a while to run</p> <p>The pipeline run may take anywhere from 10-25 minutes to complete depending on the current system load. You can see the progress of your build, as well as if any errors occur, via the UI. Thus, by monitoring the UI, you can make sure things are going as planned.</p> <p>What should you do if your pipeline run ends in failure</p> <p>If your pipeline run ends in failure, please look at the <code>Failure</code> tab (immediately below this message) to get back on track (instead of the default <code>Success</code> tab).</p> SuccessFailure <p>Pipeline Run Success View Perspective:</p> <p></p> <p>Pipeline Run Details View</p> <p>In the pipeline run <code>Details</code> view, you can see the pipeline run succeeded with all tasks having a green check mark. Additionally, observe that the event listener has triggered the <code>PipelineRun</code> instead of a user this time.</p> <p>Pipeline Run Success Logs Perspective:</p> <p></p> <p>Pipeline Run Logs View 1</p> <p>In the pipeline run <code>Logs</code> view, you can also see that the pipeline run tasks all have green check marks. Looking at the last task, you can see that the that the external connection check worked and the PetClinic application is available at the route printed in the logs. Additionally, you can see via the series of tasks marked with green checks that the dev deployment ran successfully and the system cleaned it up and ran the staging deployment successfully to complete the pipeline.</p> <p></p> <p>Pipeline Run Logs View 2</p> <p>When you switch to the <code>deploy-staging</code> task logs, by clicking on the <code>task</code> on the left hand side of the <code>Logs</code> view of the pipeline run, you see this was an automated build from git since the task prints out the <code>GIT_MESSAGE</code> that you typed in your commit word for word. (Note: If you chose a different commit message that will show instead of the one displayed in the image above.).</p> <p>Your pipeline failed, here is how to get back on the happy path</p> <ol> <li> <p>Please review your <code>pipelineRun</code> and see what error caused the failure.</p> </li> <li> <p>Make changes to fix the error. (If it's unclear what is causing the error / how to fix it, please ask the instructors for help) </p> </li> <li> <p>Resend the webhook from GitHub to trigger a new <code>pipelineRun</code> with the same values as before (see images below for help)</p> <ol> <li> <p>Click on your webhook from the <code>Webhooks</code> section of the repository settings for your GitHub repository fork of the <code>spring-petclinic</code> repository</p> <p></p> </li> <li> <p>Click on the 3 dots for the most recent delivery</p> </li> <li> <p>Click <code>Redeliver</code></p> <p></p> </li> <li> <p>Confirm Redelivery</p> <p></p> </li> </ol> </li> </ol> </li> </ol>"},{"location":"cloud-lab/application-promotion/action/#see-changes-in-application","title":"See Changes in Application","text":"<ol> <li> <p>Navigate to the <code>Topology</code> view and open a new tab with your recently deployed <code>staging</code> version of the PetClinic application by clicking <code>Open URL</code>.</p> <p></p> </li> <li> <p>Navigate to the <code>Find Owners</code> tab</p> <p></p> </li> <li> <p>Choose to add a new owner</p> <p></p> </li> <li> <p>Add the owner with details of your choice</p> <p></p> </li> <li> <p>Choose to add one of the owner's pets</p> <p></p> </li> <li> <p>Fill in the pet's details and select the new type of pet you added (turtle for the example)</p> <p></p> </li> <li> <p>View the newly created pet of the new type (Yertle the turtle for the example)</p> <p></p> </li> </ol>"},{"location":"cloud-lab/application-promotion/action/#summary","title":"Summary","text":"<p>In this section, you made a change to your PetClinic application to add a new pet type of your choice and pushed the change to GitHub. This triggered a new pipeline run which built a new image for the application tagged with the git commit hash and displayed the commit message explaining the change the build was implementing. Next, your pipeline deployed this change to OpenShift in development, tested it internally and externally and then rolled it out to staging (where it was also tested automatically). Finally, you visited the application and used the new feature (new type of pet) by adding a pet of that type to a new owner successfully. In other words, you are off the ground and running with \"cloud native\" CI/CD for your PetClinic application on IBM Z/LinuxONE! Congratulations!!!</p>"},{"location":"cloud-lab/application-promotion/git/","title":"Integrating OpenShift Pipelines with GitHub","text":"<p>It's time to add the <code>C</code> (continuous) to your CI/CD pipeline.</p>"},{"location":"cloud-lab/application-promotion/git/#add-a-github-trigger","title":"Add a GitHub Trigger","text":"<ol> <li> <p>Choose <code>Add Trigger</code> from the pipeline menu</p> <p></p> </li> <li> <p>Configure the trigger as follows (copy and paste boxes below image) and click <code>Add</code> to add the trigger to your pipeline:</p> <p></p> <p>Note</p> <p>The <code>Git_Repo</code> parameter should have your GitHub username instead of <code>siler23</code>. This should already be correctly filled out for you, so please don't change that to <code>siler23</code>.</p> Git Provider Type<pre><code>github-push\n</code></pre> <p>Note</p> <p><code>github-push</code> is in a menu you need to select from</p> GIT_MESSAGE<pre><code>$(tt.params.git-commit-message)\n</code></pre> COMMIT_SHA<pre><code>$(tt.params.git-revision)\n</code></pre> </li> </ol> <p>You are choosing the <code>github-push</code> cluster trigger binding, which is defined out of the box for OpenShift Pipelines. This passes information into a number of different variables which you can list by clicking the expand arrow seen in the picture (It will initially say <code>Show Variables</code> and then switch to <code>Hide Variables</code> when expanded as shown in the picture). You will be using the variables in green boxes in the picture to pass the git commit message (<code>git-commit-message</code>) as well as the SHA of the git commit (<code>git-revision</code>) to the build pipeline from the GitHub webhook that triggers the build.</p>"},{"location":"cloud-lab/application-promotion/git/#setting-up-git-webhook","title":"Setting up Git Webhook","text":"<p>Now, you need to set up a webhook from GitHub. You want this to hit your <code>event listener</code>, the pipelines resource which listens for events from outside sources in order to trigger a build. The listener you set up is using the <code>github-push</code> trigger binding to trigger a new pipeline run for your <code>spring-petclinic</code> pipeline passing the <code>github-push</code> parameters mentioned before. You created this <code>event-listener</code> via the OpenShift Pipelines UI when you added a trigger and will see it in the <code>Topology</code> section of the OpenShift UI as another application when you travel back there later. In order to setup your webhook to send a message to the <code>event listener</code> after a git push, do the following:</p> <ol> <li> <p>Get the event listener url from the <code>Details</code> view of your pipeline</p> <p></p> <p>Find the value listed for your pipeline and copy that value.</p> </li> <li> <p>Navigate to your git fork of the <code>github.com/ibm-wsc/spring-petclinic</code> GitHub repository</p> <p>Tip</p> <p>Your git fork should be in the form github.com/yourusername/spring-petclinic where yourusername is your GitHub username</p> </li> <li> <p>Go to the <code>settings</code> page of the repository</p> </li> <li> <p>Go to the Webhooks section and add a webhook with: </p> <ol> <li> <p><code>event listener URL</code> as the <code>PAYLOAD_URL</code></p> </li> <li> <p><code>application/json</code> selected as the <code>Content type</code></p> </li> <li> <p><code>Just the push event</code> selected for <code>Which events would you like to trigger this webhook?</code>.</p> </li> </ol> <p></p> </li> <li> <p>See the successfully created webhook now listed</p> <p></p> </li> </ol>"},{"location":"cloud-lab/application-promotion/git/#summary","title":"Summary","text":"<p>You created a GitHub webhook for your <code>spring-petclinic</code> repository fork that will trigger a new run of your <code>spring-petclinic</code> pipeline when new code is pushed to your GitHub repo1. You will trigger your pipeline via GitHub in the next section.</p> <ol> <li> <p>A more detailed explanation is that when new code is pushed to your GitHub repo, the GitHub webhook will send a payload to the event listener which then interacts with a number of OpenShift Pipelines-associated Kubernetes custom resources that you created when you used the <code>Add Trigger</code> button in the UI. Namely, the event listener will trigger a new <code>PipelineRun</code> of your <code>spring-petclinic</code> pipeline based on the <code>spring-petclinic</code> <code>TriggerTemplate</code> passing it the values for the git commit SHA hash and the commit message using the variables populated via the <code>github-push</code> <code>ClusterTriggerBinding</code>.\u00a0\u21a9</p> </li> </ol>"},{"location":"cloud-lab/application-promotion/promote/","title":"Automatically Testing and Promoting your Application","text":"<p>Here you will edit your pipeline to test your application in development, clean up your development resources, promote your application to staging, and test it in staging.</p>"},{"location":"cloud-lab/application-promotion/promote/#testing-your-application-in-the-wild","title":"Testing your Application in the Wild","text":"<p>During the build stage of your pipeline, you tested two things:</p> <ol> <li>That the pieces of your application worked (unit testing)</li> <li>That they worked together (integration testing)</li> </ol> <p>Now, it's time to go a step further and automate testing that your application is working and accessible when deployed in a real (OpenShift) environment:</p> <ol> <li>Internally (within OpenShift)</li> <li>Externally (the outside world)</li> </ol>"},{"location":"cloud-lab/application-promotion/promote/#internally-within-kubernetesopenshift","title":"Internally (Within Kubernetes/OpenShift)","text":"<p>The first thing you need to test is that the application is alive and available from within your cluster (Kubernetes environment). This is important not only for when running the CI/CD pipeline, but also for any time your application is running  (downtime is detrimental, especially in production). </p> <p>This functionality is available in Kubernetes via probes. There are 3 different types of probes to test the different aspects of your application's availability:</p> <p>Kubernetes Probes in Spring</p> <p>In Spring there are built-in endpoints for Kubernetes probes. If you are interested in learning how to program these into a Spring application of yours in the future, please take a look at Spring's official blog.</p> <ol> <li> <p>Startup probes:</p> <ol> <li> <p>Activate first</p> </li> <li> <p>Make sure an application is up and running (started up) </p> </li> <li> <p>Free startup concerns/constraints from other probes</p> </li> </ol> <p>Here is the <code>startupProbe</code> for the container running the PetClinic application:</p> <pre><code>startupProbe:\nhttpGet:\npath: /actuator/health/liveness\nport: 8080\nperiodSeconds: 10\nfailureThreshold: 30\n</code></pre> <p>It simply queries (via localhost) PetClinic's liveness health endpoint. Once this returns successfully, you can be confident the application has started up and can begin to monitor the liveness and readiness of each container of each replica (pod) of your application throughout its lifecycle.</p> </li> <li> <p>Liveness probes:</p> <ol> <li> <p>Make sure an application is actually running and not caught in a deadlock (it's alive)</p> </li> <li> <p>Restart \"dead\" containers automatically with kubelet</p> </li> <li> <p>Fix problems that may arise in long-running containers via the aforementioned restart</p> </li> </ol> <p>Here is the <code>livenessProbe</code> for the container running the PetClinic application:</p> <pre><code>livenessProbe:\nhttpGet:\npath: /actuator/health/liveness\nport: 8080\nperiodSeconds: 10\nfailureThreshold: 3\n</code></pre> <p>This looks almost identical to the <code>startupProbe</code> above other than having a much lower <code>failureThreshold</code>. The <code>startupProbe</code> is making sure the container of a given pod of your application's deployment is alive when it first starts up (It is allowing time for that startup to occur). On the other hand, the <code>liveness</code> probe above is making sure your application stays alive throughout its lifecycle. Therefore, it has a much lower <code>failureThreshold</code> to enable kubelet to quickly respond (restart the container) when the container becomes deadlocked.</p> </li> <li> <p>Readiness probes:</p> <ol> <li> <p>Check if each copy (replica) of an application is ready</p> </li> <li> <p>Makes sure traffic goes only to replicas that are ready for it</p> </li> <li> <p>Prevents users from interacting with unready replicas (getting unnecessary errors)</p> </li> </ol> <p>Here is the <code>readinessProbe</code> for the container running PetClinic:</p> <pre><code>readinessProbe:\nhttpGet:\npath: /actuator/health/readiness\nport: 8080\nperiodSeconds: 10\n</code></pre> <p>It simply queries (via localhost) PetClinic's readiness health endpoint. This probe will let Kubernetes know when to send traffic to a PetClinic replica. When you send traffic to the application, only the available replicas will receive it. This means that replicas which aren't ready for traffic don't accidentally get it, preventing errors for the user.</p> </li> </ol> <p>These 3 probes serve to declare to Kubernetes the way your application (and the replicas that make it up) should behave, enabling the system to monitor and take action on your behalf (restarting the container or removing its pod's endpoint from service) when the current state (the status) does not meet the desired state (your specification).</p> <p>The rollout task you created before as <code>deploy-dev</code> will only complete once all desired replicas are ready, implying that both the <code>startup</code> (initial liveness) and <code>readiness</code> probes have successfully passed and all replicas of your application are initially alive and ready for business. </p>"},{"location":"cloud-lab/application-promotion/promote/#testing-external-connections","title":"Testing External Connections","text":"<p>While making sure your application is internally up and running is important, at the end of the day you want to provide access to your users externally1. </p> <p>This means it is important to also test the OpenShift route (the component providing the external connection) as part of your CI/CD pipeline to ensure it is correctly servicing web traffic external to your cluster2.</p>"},{"location":"cloud-lab/application-promotion/promote/#create-external-route-test-task","title":"Create External Route Test Task","text":"<p>You will create a task to check the connection to your external route as part of your CI/CD pipeline.</p> <ol> <li> <p>Copy the <code>connection-test</code> task using the following definition (copy by clicking on the copy icon in the top right of the box below):</p> <p><pre><code>apiVersion: tekton.dev/v1beta1\nkind: Task\nmetadata:\nname: connection-test\nspec:\ndescription: &gt;-\n\"This task runs a bash script to determine if a given application\nis accessible to the outside world via its route.\"\nparams:\n- name: ROUTE_NAME\ndefault: \"\"\ndescription: \"The name of the OpenShift route for the application.\"\ntype: string\n- name: APP_PATH\ndefault: \"/\"\ndescription: \"The path to reach the application from it's hostname\"\ntype: string\n- name: EXPECTED_STATUS\ndefault: \"200\"\ndescription: \"The expected http(s) status code from querying the application.\"\ntype: string\n- name: TIMEOUT\ndefault: \"30\"\ndescription: \"The number of seconds to try before giving up on a successful connection.\"\ntype: string\n- name: SECURE_CONNECTION\ndefault: \"true\"\ndescription: \"true for a secure route (https), false for an insecure (http) route.\"\ntype: string\nsteps:\n- name: route-connection-test\nimage: 'image-registry.openshift-image-registry.svc:5000/openshift/cli:latest'\nresources:\nlimits:\ncpu: 200m\nmemory: 200Mi\nrequests:\ncpu: 200m\nmemory: 200Mi\nscript: |\n#!/usr/bin/env bash\n# Make parameters into variables for clarity\nexport route_name=\"$(params.ROUTE_NAME)\"\nexport expected_status=\"$(params.EXPECTED_STATUS)\"\nexport app_path=\"$(params.APP_PATH)\"\nexport timeout=\"$(params.TIMEOUT)\"\nexport secure_connection=\"$(params.SECURE_CONNECTION)\"\n\n# If true, http(s), if false (or otherwise) http\nif [ \"${secure_connection}\" == \"true\" ]\nthen\nexport header=\"https://\"\necho \"Using secure https connection...\"\nelse\nexport header=\"http://\"\necho \"Using insecure http connection...\"\nfi\n# Start timer at 0\nSECONDS=0\n# Once timeout reached, stop retrying\nwhile [ \"${SECONDS}\" -lt \"${timeout}\" ];\ndo\n# Get hostname of route\nhostname=\"$(oc get route ${route_name} -o jsonpath='{.spec.host}')\"\n# Get http(s) status of web page via external connection (route)\nstatus=$(curl -s -o /dev/null -w \"%{http_code}\" \"${header}${hostname}${app_path}\")\n# Print test completion message if expected status code received\nif [ \"${status}\" -eq \"${expected_status}\" ]\nthen\necho \"---------------------------TESTS COMPLETE---------------------------\"\necho \"Congratulations on a successful test!\"\necho \"Please visit the application at:\"\necho\necho \"${header}${hostname}${app_path}\"\nexit 0\n# Print failure message if incorrect status code received + retry\nelse\necho \"The application is unexpectedly returning http(s) code ${status}...\"\necho \"It is not available to outside traffic yet...\"\necho \"Retrying in 5s at:\"\necho\necho \"${header}${hostname}${app_path}\"\nsleep 5\nfi\ndone\n# Redirect output to standard error, print message, and exit with error after timeout\n&gt;&amp;2 echo \"Error, failed after ${timeout} seconds of trying...\"\n&gt;&amp;2 echo \"The application was never accessible to the outside world :(\"\nexit 1\n</code></pre> 2. Create the <code>connection-test</code> Task</p> <ol> <li> <p>Click <code>Import YAML</code> to bring up the box where you can create Kubernetes resource definitions from yaml</p> </li> <li> <p>Paste the <code>connection-test</code> Task into the box</p> </li> <li> <p>Scroll down and click create to create the <code>connection-test</code> Task </p> </li> </ol> <p></p> </li> </ol> <p>You should now see the created <code>connection-test</code> Task. Finally, navigate back to the <code>Pipelines</code> section of the OpenShift UI and go back to editing your pipeline.</p> <p></p>"},{"location":"cloud-lab/application-promotion/promote/#add-external-route-test-task-to-pipeline","title":"Add External Route Test Task to Pipeline","text":"<ol> <li> <p>Add a sequential task after <code>deploy-dev</code>. When you <code>Select Task</code>, choose the <code>connection-test</code> task. </p> <p></p> </li> <li> <p>Configure <code>connection-test</code> task</p> <p>The only values you need to change are the <code>Display Name</code> and the <code>ROUTE_NAME</code> (copy and paste boxes below image):</p> <p></p> Display Name<pre><code>connect-dev\n</code></pre> ROUTE_NAME<pre><code>spring-petclinic-dev\n</code></pre> </li> <li> <p><code>Save</code> the pipeline</p> </li> </ol> <p>Your current pipeline builds and tests your application, creates a docker image for it, deploys it to the development environment, and ensures that the application is working both internally and externally. In other words, once your application successfully completes the current pipeline, you can be confident in it and be ready to move to staging3. </p>"},{"location":"cloud-lab/application-promotion/promote/#deploy-staging","title":"Deploy Staging","text":"<p>Moving to the staging environment means spinning up your application in that environment (with parameters relevant for it) and testing it there. Given that this is all using containers, you can easily free up the development resources that have successfully completed and then spin up the new resources in your staging environment.</p>"},{"location":"cloud-lab/application-promotion/promote/#remove-dev","title":"Remove Dev","text":"<p>Your first Task will mirror the <code>cleanup-resources</code> task at the beginning of your pipeline, but will just cleanup the <code>dev</code> resources using the <code>env=dev</code> label selector.</p> <ol> <li> <p>Go back to editing your pipeline via <code>Actions -&gt; Edit Pipeline</code></p> <p></p> </li> <li> <p>Add a Task sequentially at the end of the pipeline (after <code>connect-dev</code>) using the <code>openshift-client</code> ClusterTask.  </p> <p></p> </li> <li> <p>Configure the Task with the following values (copy and paste boxes below image):</p> <p></p> Display Name<pre><code>cleanup-dev\n</code></pre> SCRIPT<pre><code>oc delete deployment,cm,svc,route -l app=spring-petclinic,env=dev --ignore-not-found\n</code></pre> <p>and an empty <code>ARGS</code> value.</p> <p>No help please!</p> <p>Make sure <code>help</code> is deleted from the <code>ARGS</code> section (click the - button to delete the default help args line). </p> </li> </ol>"},{"location":"cloud-lab/application-promotion/promote/#add-staging","title":"Add Staging","text":"<p>You will use your existing <code>kustomize</code> task to deploy the staging configuration for your PetClinic application in a new <code>kustomize-staging</code> task. Customizations for staging PetClinic include adding a staging environment label, name suffix, change cause, and staging environment variables for your application. You could deploy to a separate project or cluster altogether as well as change replicas or add pod autoscalers in a similar manner (depending on your use case) for different environments. </p> <ol> <li> <p>Add a <code>kustomize</code> task sequentially to the end of your current pipeline (after <code>cleanup-dev</code>)</p> <p> </p> </li> <li> <p>Configure the Task with the following values (copy and paste boxes below image):</p> <p></p> Display Name<pre><code>kustomize-staging\n</code></pre> RELEASE_SUBDIR<pre><code>overlay/staging\n</code></pre> SCRIPT<pre><code>kustomize edit set image spring-petclinic=$(params.IMAGE_NAME)-minimal:$(params.COMMIT_SHA)\n</code></pre> SOURCE (choose from dropdown)<pre><code>workspace\n</code></pre> </li> </ol>"},{"location":"cloud-lab/application-promotion/promote/#rollout-staging","title":"Rollout Staging","text":"<ol> <li> <p>Edit the pipeline again and add a <code>deploy-staging</code> task with the <code>openshift-client</code> ClusterTask</p> <p></p> </li> <li> <p>Configure the task with the following parameters4 (copy and paste boxes below image):</p> <p></p> Display Name<pre><code>deploy-staging\n</code></pre> SCRIPT<pre><code>echo \"$(params.GIT_MESSAGE)\" &amp;&amp; oc rollout status deploy/spring-petclinic-staging\n</code></pre> <p>No help please!</p> <p>Make sure <code>help</code> is deleted from the <code>ARGS</code> section (click the - button to delete the default help args line).</p> </li> </ol>"},{"location":"cloud-lab/application-promotion/promote/#add-external-route-test-task-to-pipeline_1","title":"Add External Route Test Task to Pipeline","text":"<ol> <li> <p>Add a sequential task after <code>deploy-staging</code>. When you <code>Select Task</code>, choose the <code>connection-test</code> task. </p> <p></p> </li> <li> <p>Configure <code>connection-test</code> task with the following parameters (copy and paste boxes below image):</p> <p></p> Display Name<pre><code>connect-staging\n</code></pre> ROUTE_NAME<pre><code>spring-petclinic-staging\n</code></pre> </li> <li> <p><code>Save</code> the pipeline</p> <p></p> </li> </ol>"},{"location":"cloud-lab/application-promotion/promote/#summary","title":"Summary","text":"<p>Congratulations! You have built a pipeline that tests your <code>PetClinic</code> application, creates a docker image for it, deploys it to the development environment with dev configuration, ensures that the application is working both internally and externally, cleans up the development environment, deploys it to the staging environment with staging configuration, and then makes sure it is working both internally and externally5. </p> <p>tl;dr</p> <p>You now have the <code>I/D</code> (Integration/Deployment) in <code>CI/CD</code>6.</p> <ol> <li> <p>For different environments like dev and test, this may be different groups external to your Kubernetes environment (cluster), though internal to the organization itself and accessing the endpoints via a VPN or internal network. Production is likely when external connection via an organization's real website would happen. The type of external connection (via a VPN or public connection) has little impact on the Kubernetes resources given a route will be used for all of those types of external connections (the most important thing is that the route you are testing is available to you [the tester] from where you are).\u00a0\u21a9</p> </li> <li> <p>You may think to yourself that you can't test an external connection from inside your cluster. However, by using the route, you are causing the traffic to go \"outside\" the cluster's networking to reach the load balancer and then back \"inside\" via the route. This explicitly tests the external connection and makes sure that it indeed works. However, you are just testing that the route works, not that the dns/hostname is available generally on the internet or private enterprise subnet (depending on environment). Internet / subnet dns resolution is a different, more general problem for your networking team (or cloud provider) to ensure for all of the applications using that network.\u00a0\u21a9</p> </li> <li> <p>You could create more extensive tests to make sure that the pages are rendering correctly (besides just returning a proper status code). However, that is beyond the scope of the lab and this at least makes sure requests are successfully sent and returned via an external route, which is good enough for the lab's purposes.\u00a0\u21a9</p> </li> <li> <p>This mirrors the <code>dev-deploy</code> task which waits for the dev release to rollout but uses the <code>SCRIPT</code> field for everything vs. <code>ARGS</code>.\u00a0\u21a9</p> </li> <li> <p>You could clean up the staging environment at the end of the run but choose not to so that the user can interact with it between runs. You could also clean up or use a separate MySQL instance for staging but due to limited resources in your environment you have chosen not to add this extra component.\u00a0\u21a9</p> </li> <li> <p>You'll add the double <code>C</code>s in the next section by connecting it to GitHub.\u00a0\u21a9</p> </li> </ol>"},{"location":"cloud-lab/build-and-deploy/","title":"It's Time to Open up your Pet Clinic for Testing","text":"<p>In this section, you will build, test and deploy the Java web application for your pet clinic (PetClinic) to get it up and running on OpenShift. This involves the following tasks:</p> <ul> <li>Deploying MySQL database</li> <li>Building and deploying PetClinic with automated testing</li> <li>Accessing PetClinic and adding an owner</li> </ul>"},{"location":"cloud-lab/build-and-deploy/upandrunning/","title":"Getting Your PetClinic Application Up and Running","text":"<p>For this workshop you will be using the iconic Spring PetClinic application. The Spring PetClinic is a sample application designed to show how the Spring stack can be used to build simple, but powerful database-oriented applications. The official version of PetClinic demonstrates the use of Spring Boot with Spring MVC and Spring Data JPA. </p> <p>You will not be focusing on the ins and outs of the PetClinic application itself, but rather on leveraging OpenShift tooling to build a PetClinic cloud native application and a DevSecOps pipeline for the application.</p> <p>You will start by building your PetClinic application from the source code and connecting it to a MySQL database.</p> <p>Lab Guide</p> <ul> <li>For the images in this lab:</li> <li>the green arrows or boxes denote something to look at or reference </li> <li>the red arrows or boxes denote something to click on or type.</li> </ul> <p>Using LinuxONE Community Cloud</p> <p>Because you are using the LinuxONE Community Cloud OpenShift trial, your project name will be different from the project name depicted in the diagrams below. You will be operating in your assigned project for the entirety of the lab.</p>"},{"location":"cloud-lab/build-and-deploy/upandrunning/#logging-into-your-linuxone-community-cloud-openshift-environment","title":"Logging into your LinuxONE Community Cloud OpenShift environment","text":"<p>1. Please complete the Prerequisites and register for a LinuxONE Community Cloud OpenShift trial if you have not done so already.</p> <p>2. After you register, you should have received an email asking you to Activate your account or entitlement. Please do so now if you have not already done so. The link to activate your trial is only valid for 48 hours after your initial registration. If this time period has passed you need to re-register for the trial again.</p> <p></p> <p>3. After activation, log into your LinuxONE Community Cloud account using the link here.</p> <p>4. You should see the Topology view of the OpenShift console. Click on  your project name (it will be a number randomly assigned to you that will be different than the one shown in the picture below):</p> <p></p>"},{"location":"cloud-lab/build-and-deploy/upandrunning/#deploying-mysql-database","title":"Deploying MySQL database","text":"<ol> <li> <p>First, you need to setup your mysql database. Luckily, this is very easy on OpenShift with the mysql template available from the main developer topology window. Follow the steps in the diagram below to bring up the available database options. (Note your project name will be different than the picture below)</p> <p></p> <p>Now you can start the lab!</p> </li> <li> <p>Next, select the <code>MySQL (Ephemeral)</code> tile. </p> <p>Why Ephemeral?</p> <p>You are using the Ephemeral implementation because this a short-lived demo and you do not need to retain the data.  In a staging or production environment, you will most likely be using a MySQL deployment backed by a Persistent Volume Claim. This stores the data in a Persistent Volume (basically a virtual hard drive), and the data will persist beyond the life of the container.</p> <p></p> </li> <li> <p>Click on instantiate template.</p> <p></p> </li> <li> <p>Fill the wizard with the parameters as shown in the image below (your namespace will be different from the image below):</p> <p></p> <pre><code>petclinic\n</code></pre> <p>Click the <code>Create</code> button. </p> </li> <li> <p>A minute or two later, in the <code>Topology</code> view of your OpenShift Console, you should see <code>mysql</code> in the <code>Running</code> state. (Click on the Topology icon for <code>mysql</code> to bring up the side panel)</p> <p></p> </li> </ol>"},{"location":"cloud-lab/build-and-deploy/upandrunning/#fork-the-petclinic-repo-to-your-own-github-account","title":"Fork the PetClinic repo to your own GitHub account","text":"<p>For this workshop, you will be using the PetClinic application from your own GitHub account so that you can enable integrations with it later.</p> <p>To make a copy of the PetClinic application into your GitHub account, click here</p> <p>At this point, you might need to log into GitHub if you weren't logged in already.</p> <p>Next, you will be presented with a fork menu like the following:</p> <p></p> <ol> <li> <p>Select your own user account from the <code>Owner</code> dropdown</p> </li> <li> <p>Uncheck the box <code>Copy the main branch only</code></p> </li> </ol> <p>Please make a note of your repo URL for later. It should be something like:</p> <p><code>https://github.com/&lt;your-github-username&gt;/spring-petclinic</code></p> <p>That's it! You are ready to move on to the next section.</p>"},{"location":"cloud-lab/build-and-deploy/upandrunning/#building-and-deploying-petclinic-application","title":"Building and Deploying PetClinic Application","text":"<p>There are multiple ways OpenShift enables cloud native application developers to package up their applications and deploy them. For PetClinic, you will be building your container image from source, leveraging OpenShift's S2I (Source to Image) capability. This allows you to quickly test the building, packaging, and deployment of your application, and gives you the option to create and use a DevSecOps pipeline from this workflow. It's a good way to start to understand how OpenShift Pipelines work.</p> <p>1. Start with choosing Add From Git:</p> <p></p> <p>2. Enter <code>https://github.com/&lt;your-github-ID&gt;/spring-petclinic</code> in the <code>Git Repo URL</code> field. Expand the <code>Show Advanced Git Options</code> section, and type in <code>main</code> for the <code>Git Reference</code>. This tells OpenShift which GitHub repo and branch to pull the source code from.</p> <p></p> <p>3. Scroll down to the <code>Builder</code> section. Select the <code>OpenJ9</code> tile and select <code>openj9-11-el8</code> as the builder container image version. As you can see OpenShift offers many different builder container images to help you build container images from a variety of programming languages. Your list of builder container images might differ from the screen shot. For Java on Z, the recommended JVM is <code>OpenJ9</code> because it has built-in s390x optimizations as well as container optimizations.</p> <p></p> <p>4. In the General section, put in the following entries for Application Name and Name. </p> <p></p> <p>5. Scroll down to the  Pipelines section, select the checkbox next to <code>Add pipeline</code>. You can also expand the <code>Show pipeline visualization</code> section to see a visual of the build pipeline.</p> <p></p> <p>6. You are almost there! You will need to configure a couple of Advanced Options. First, click on <code>Show advanced Routing options</code> in the Advanced Options section to expand the Routing options.</p> <p></p> <p>7. In the Routing options section, only fill out the Security options as follows. You can leave the rest alone. These options will enable only TLS access to your PetClinic application.</p> <p></p> <p>8. You are done with configurations of this panel. Scroll all the way down and hit the <code>Create</code> button which will kick off the pipeline build of your PetClinic application. In a few seconds you will see your Topology with the new application icon. Hit the little pipeline icon in the diagram below to view the build logs.  You might see errors associated with ImageStream not being able to pull the application image during the build process. This does not mean that the build has failed. The pipeline creates the ImageStream first and then goes through the actual build process, and since the build process takes 10-15 minutes to complete, this error will be there until then. </p> <p></p> <p>Log Streaming Gotcha in the LinuxONE CC</p> <p>PLEASE BEWARE that if you are using the LinuxONE Community Cloud OpenShift Trial you might see lag with the log streaming. If it stops streaming, you might want to go back out to the <code>Topology</code> view. You can always return to the logs view, once the pipeline completes, to see the logs.</p> <p>9. The pipeline will go through three tasks:</p> <p>\u00a0\u00a0\u00a0 1. fetch-repository - this Pipeline task will git clone your PetClinic repo for the build task.</p> <p>\u00a0\u00a0\u00a0 2. build - this Pipeline task is the build process which itself is broken down into a few sub-steps. This is the longest task in the pipeline, and can take up to 15 minutes. The steps that it goes through are as follows:</p> <p>build steps</p> <ul> <li>STEP-GEN-ENV-FILE: this step generates the environment file to be used during the build process</li> <li>STEP-GENERATE: this step generates the Dockerfile that will be used to create the OCI container image later on during the build step</li> <li>STEP-BUILD: this is the multi-step build process of creating an OCI container image out of your Java application PetClinic. It will download the required Maven Java packages, compile the Java application, run through a set of 39 unit tests on the application, and finally build the application jar file and the OCI container image. If the tests fail, this step will not complete.</li> <li>STEP-PUSH: this final step pushes the built OCI container image to the OpenShift image registry.</li> </ul> <p>\u00a0\u00a0\u00a0 3. deploy - this Pipeline task will deploy the newly built container image as a running deployment in your project. After this, your application will be running in a pod and be accessible via a route.</p> <p>Below is an image of the log of a successful build task: </p> <p></p> <p>10. Now if you go back to the Topology view, you should see the application has been successfully deployed to OpenShift as well. From here you can click on the <code>open URL</code> circle, and a new browser tab should open to lead you to your PetClinic's front page.  It can take a couple of minutes before the application is accessible through its URL so if it doesn't come up right away wait a few minutes and try again. </p> <p></p>"},{"location":"cloud-lab/build-and-deploy/upandrunning/#interacting-with-your-petclinic-application-and-mysql-database","title":"Interacting with Your PetClinic Application and MySQL database","text":"<p>In this section, you will add a new owner to the Pet Clinic application, and then go into your MySQL container to see if that owner was successfully added. </p> <p>1. Your Pet Clinic should look something similar to this. Go to the Find Owners tab, and create a new owner.</p> <p></p> <p>2. Click on the <code>Add Owner</code> button, and add an owner of your own, for example:</p> <p></p> <p>3. You can then go back to <code>Find Owners</code> and try searching for the owner that you just added. It should come back with the search results similar to the following.</p> <p></p> <p>4. Now let's check the MySQL database to make sure that the new owner you just added is in there.</p> <p>Return to your OpenShift console, from the Topology view, click on the <code>mysql</code> icon. This will bring up a side panel, and then click on the <code>mysql</code> pod (your pod name will be different than the picture):</p> <p></p> <p>In the pod panel, go to the Terminal tab.</p> <p></p> <p>Now type in the following commands in your <code>mysql</code> terminal (copy and paste box below image):</p> <p></p> <pre><code>mysql -u root -h mysql -ppetclinic\n</code></pre> <pre><code>use petclinic;\n</code></pre> <pre><code>show tables;\n</code></pre> <p>Let's run a SQL command now to verify that the owner that you added through the application is indeed in the database (copy and paste box below image):</p> <p></p> <pre><code>select * from owners;\n</code></pre> <p>Tip</p> <p>If you added a different user than alice you should see that user in place of alice on your screen.</p> <p>Please let the instructors know, if you don't see your owner you added listed. </p>"},{"location":"cloud-lab/build-and-deploy/upandrunning/#summary","title":"Summary","text":"<p>Congratulations, you have completed this part of the workshop! You have your virtual pet clinic up and running and have created an OpenShift Pipelines pipeline that you will build on in the next sections of the lab to achieve CI/CD. You may move on to the next part by clicking <code>Next</code> on the bottom right of the page.</p>"},{"location":"cloud-lab/cleanup/full-cleanup/","title":"Environment Cleanup","text":"<p>In this section you will clean up the different things you made during the lab in order to free up resources for other projects you intend to embark on in the community cloud as well as for other users of the environment.</p>"},{"location":"cloud-lab/cleanup/full-cleanup/#pipelines-section-cleanup","title":"Pipelines Section Cleanup","text":"<p>From the <code>Pipelines</code> section of the OpenShift UI, please complete the following cleanup tasks:</p> <ol> <li> <p>Delete the trigger for your pipeline</p> <ol> <li> <p>Choose to remove the trigger</p> <p></p> <ol> <li>Click the 3 dots</li> <li>Choose <code>Remove Trigger</code></li> </ol> </li> <li> <p>Confirm the trigger removal</p> <p></p> <ol> <li>Choose your trigger from the dropdown menu</li> <li>Click <code>Remove</code></li> </ol> </li> </ol> </li> <li> <p>Delete the pipeline</p> <p></p> <ol> <li>Click the 3 dots</li> <li>Choose <code>Delete Pipeline</code></li> </ol> </li> </ol>"},{"location":"cloud-lab/cleanup/full-cleanup/#topology-section-cleanup","title":"Topology Section Cleanup","text":"<p>From the <code>Topology</code> section of the OpenShift UI, please complete the following cleanup tasks:</p> <ol> <li> <p>Delete the <code>spring-petclinic-staging</code> deployment and its associated resources</p> <ol> <li> <p>Right-click on the icon and choose <code>Delete Deployment</code></p> <p></p> </li> <li> <p>Click <code>Delete</code> (keep box checked to also delete dependent objects of this resource)</p> <p></p> </li> </ol> </li> <li> <p>Delete <code>mysql</code> deployment config and its associated resources</p> <ol> <li> <p>Right-click on the icon and choose <code>Delete DeploymentConfig</code></p> <p></p> </li> <li> <p>Click <code>Delete</code> (keep box checked to also delete dependent objects of this resource)</p> <p></p> </li> </ol> </li> </ol>"},{"location":"cloud-lab/cleanup/full-cleanup/#delete-leftover-resources","title":"Delete Leftover Resources","text":"<ol> <li> <p>Click on the <code>Search</code> tab from the OpenShift menu</p> <p></p> </li> <li> <p>Click on the <code>Resources</code> drop down menu</p> <p></p> </li> <li> <p>Check (click the checkbox) the following resources (you can search for them individually) </p> <ol> <li> <p>Secret</p> <pre><code>Secret\n</code></pre> </li> <li> <p>Route (route.openshift.io/v1)</p> <pre><code>Route\n</code></pre> </li> <li> <p>Service (core/v1)</p> <pre><code>Service\n</code></pre> </li> <li> <p>ImageStream</p> <pre><code>ImageStream\n</code></pre> </li> <li> <p>ConfigMap</p> <pre><code>ConfigMap\n</code></pre> </li> <li> <p>PersistentVolumeClaim</p> <pre><code>PersistentVolumeClaim\n</code></pre> </li> </ol> <p></p> </li> <li> <p>Select <code>Name</code> for the filter</p> <p></p> </li> <li> <p>Delete the resources for <code>mysql</code></p> <ol> <li> <p>Search for the Name <code>mysql</code></p> <pre><code>mysql\n</code></pre> <p></p> </li> <li> <p>Click on the 3 dots to the right of the first individual resource</p> <p></p> </li> <li> <p>Confirm the deletion in the following window</p> <p></p> </li> <li> <p>Repeat this for all of the other resources that appear for <code>mysql</code></p> <p>Tip<p>This should include 2 secrets (<code>mysql</code> and one starting with <code>mysql-ephemeral-parameters-</code>) and 1 <code>mysql</code> service.</p> </p> </li> </ol> </li> <li> <p>Delete the resources for the Name <code>spring-petclinic</code></p> <pre><code>spring-petclinic\n</code></pre> <p></p> <p>Tip<p>This should include 2 secrets, 1 route, 1 service, 2 imageStreams, and 2 configMaps.</p> </p> </li> <li> <p>Delete the resources for the Name <code>event-listener</code></p> <pre><code>event-listener\n</code></pre> <p></p> <p>Tip<p>This should include 1 route.</p> </p> </li> <li> <p>Delete the <code>persistentVolumeClaim</code> associated with your pipeline</p> <ol> <li> <p>Leave the Name field blank and go to the PersistentVolumeClaim section of the page</p> </li> <li> <p>Delete the <code>persistentVolumeClaim</code> (if there are more than 1, delete the 1 created for this lab [you can look at the creation time to double check this])</p> </li> </ol> <p>Tip<p>This should include 1 <code>persistentVolumeClaim</code>.</p> </p> </li> </ol>"},{"location":"cloud-lab/cleanup/full-cleanup/#github-section-cleanup","title":"GitHub Section Cleanup","text":"<p>Finally, you will cleanup the GitHub fork you made on your GitHub account with the following steps:</p> <ol> <li> <p>Navigate to the settings for your forked GitHub repository</p> <p></p> </li> <li> <p>Scroll to the bottom of the settings page (to the <code>danger zone</code>) and choose to delete your repository</p> <p></p> </li> <li> <p>Confirm repository delete (retyping your forked repository's name)</p> <p></p> </li> </ol>"},{"location":"cloud-lab/cleanup/full-cleanup/#thank-you-for-cleaning-up","title":"Thank You for Cleaning Up!","text":""},{"location":"cloud-lab/devsecops/devsecops/","title":"Configure SonarQube code analysis in your Pipeline","text":"<p>As a bonus lab, you will now configure an extra task in your existing Pipeline to conduct code scanning on your petclinic source code. This exercise is to show you one way of incorporating security code scanning as part of your automated CI/CD pipeline.</p> <p>We will use the popular open source package SonarQube to do the code scanning. According to Wikipedia, \"SonarQube is an open-source platform developed by SonarSource for continuous inspection of code quality to perform automatic reviews with static analysis of code to detect bugs, code smells, and security vulnerabilities on 20+ programming languages.\"</p> <p>For Petclinic, we will be using SonarScanner for Maven. The ability to execute the SonarQube analysis via a regular Maven goal makes it available anywhere Maven is available (developer build, CI server, etc.), without the need to manually download, setup, and maintain a SonarQube Runner installation. For more information on SonarScanner for Maven, please see here.</p>"},{"location":"cloud-lab/devsecops/devsecops/#accessing-the-sonarqube-server-with-your-assigned-credentials","title":"Accessing the SonarQube server with your assigned credentials","text":"<p>The lab instructors have already setup a SonarQube server within the OpenShift cluster for you to access for code scanning. Credentials have also been setup for you. Please use your assigned credentials to test access to the SonarQube Server.</p> <p>Access the SonarQube server here</p> <p>Select <code>Log in</code> in the upper right hand corner. And log in with your assigned credentials.</p> <p>If you are not successful with this step, please let the instructor know.</p>"},{"location":"cloud-lab/devsecops/devsecops/#generate-a-security-token-for-your-sonarqube-account","title":"Generate a security token for your SonarQube account","text":"<p>You'll need either your credentials, or an access token associated with your account, in order to access the server for code scanning. </p> <p>Let's use the access token method.</p> <p>Now that you've logged in, select your account in the upper right hand corner of the SonarQube server page.</p> <p> </p> <p>In the account panel, go to the security tab, and type in the name <code>petclinic</code> to help identify your token, and then select <code>Generate</code>. Now copy and save this token to be used in the next step.</p> <p> </p>"},{"location":"cloud-lab/devsecops/devsecops/#configuring-maven-settings-with-the-sonar-scanner-plugin","title":"Configuring maven-settings with the Sonar scanner plugin","text":"<p>We need to configure maven with the Sonar scanner plugin prefix. We will do that by including the sonar scanner plugin in the maven settings file.</p> <p>We will create a Kubernetes ConfigMap for the mavens settings file.</p> <p>Click on the Import Yaml button at the top of the OpenShift console (the '+' symbol).</p> <p>Copy and paste the entirety of the following into the editor and then hit \"Create\" (copy by clicking on the copy icon in the top right of the box below).</p> <pre><code>kind: ConfigMap\napiVersion: v1\nmetadata:\n  name: maven-settings\ndata:\n  settings.xml: |\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n    &lt;settings&gt;\n      &lt;pluginGroups&gt;\n        &lt;pluginGroup&gt;io.spring.javaformat&lt;/pluginGroup&gt;\n        &lt;pluginGroup&gt;org.sonarsource.scanner.maven&lt;/pluginGroup&gt;\n      &lt;/pluginGroups&gt;\n      &lt;profiles&gt;\n        &lt;profile&gt;\n          &lt;id&gt;sonar&lt;/id&gt;\n          &lt;activation&gt;\n            &lt;activeByDefault&gt;true&lt;/activeByDefault&gt;\n          &lt;/activation&gt;\n          &lt;properties&gt;\n            &lt;!-- Wait until the quality check is complete in SonarQube  --&gt;\n            &lt;sonar.qualitygate.wait&gt;\n              true\n&lt;/sonar.qualitygate.wait&gt;\n            &lt;!-- Exclude DTO Files from SonarQube duplication check as these should have duplications --&gt;\n            &lt;sonar.cpd.exclusions&gt;\n              **/*DTO*\n            &lt;/sonar.cpd.exclusions&gt;\n          &lt;/properties&gt;\n        &lt;/profile&gt;\n      &lt;/profiles&gt;\n    &lt;/settings&gt;\n</code></pre>"},{"location":"cloud-lab/devsecops/devsecops/#configuring-maven-task-into-pipeline-to-do-code-analysis","title":"Configuring maven task into Pipeline to do code analysis","text":"<p>Go back to your OpenShift console and go to your pipeline. Your pipeline should look like the picture below, at this point of the workshop.</p> <p> </p> <ol> <li> <p>Add <code>maven-settings</code> workspace to your pipeline</p> <p></p> <ol> <li>Click <code>Add workspace</code></li> <li> <p>Name the workspace</p> <pre><code>maven-settings\n</code></pre> </li> <li> <p>Click <code>Save</code></p> </li> </ol> </li> <li> <p>We will insert the code analysis task before the build task. The idea being we want to scan the source code for bugs and vulnerabilities, before we build a container image out of it.</p> <p>a. From your pipeline screen, Go to Actions -&gt; Edit Pipeline.</p> <p>b. Select the plus sign before the build task, as in the picture below.</p> <p></p> <p>c. Then select the task <code>maven</code> from the drop down list.</p> <p></p> <p>Tip</p> <p>Once you add a specific task (i.e. <code>maven</code>), clicking on the oval of the task will enable you to edit its default values for your needs.</p> </li> <li> <p>Give the task the following parameters to do the code analysis with the proper maven goals set to do code scanning against our SonarQube server.</p> <p></p> Display Name<pre><code>code-analysis\n</code></pre> MAVEN_IMAGE<pre><code>maven:3.8.1-jdk-11-openj9\n</code></pre> <p>GOALS</p> GOAL 1<pre><code>package\n</code></pre> GOAL 2<pre><code>sonar:sonar\n</code></pre> GOAL 3<pre><code>-Dsonar.login=&lt;use-your-token-from-previous-step&gt;\n</code></pre> <p>Use your token</p> <p>You need to replace <code>&lt;use-your-token-from-previous-step&gt;</code> with your actual token.</p> GOAL 4<pre><code>-Dsonar.host.url=https://sonarqube-1677091477712.apps.cloudnative.marist.edu\n</code></pre> GOAL 5<pre><code>-Dsonar.projectName=petclinic-&lt;your-name&gt;\n</code></pre> <p>Use your name</p> <p>Be mindful to put your name in the value of the <code>Dsonar.projectName</code> and <code>`Dsonar.projectKey</code> goals (i.e., substitute <code>&lt;your-name&gt;</code> with your name such as <code>petclinic-garrett</code>).</p> GOAL 6<pre><code>-Dsonar.projectKey=petclinic-&lt;your-name&gt;\n</code></pre> SOURCE (choose from dropdown)<pre><code>workspace\n</code></pre> MAVEN-SETTINGS (choose from dropdown)<pre><code>maven-settings\n</code></pre> <p>Warning</p> <p>Remember to replace <code>&lt;your-name&gt;</code> with your name such as <code>petclinic-garrett</code>.</p> </li> <li> <p>Now you can click away to get back to the main pipeline edit panel.</p> </li> <li> <p>Save the <code>pipeline</code>.</p> </li> </ol>"},{"location":"cloud-lab/devsecops/devsecops/#add-the-new-maven-settings-workspace-to-the-triggertemplate","title":"Add the new <code>maven-settings</code> workspace to the TriggerTemplate","text":"<ol> <li> <p>Go to the TriggerTemplates section of your pipeline and click the link to take you to your pipeline's <code>TriggerTemplate</code></p> <p></p> </li> <li> <p>Edit the <code>TriggerTemplate</code></p> <ol> <li>Click Actions</li> <li>Choose <code>Edit TriggerTemplate</code> from the dropdown menu</li> </ol> <p></p> </li> <li> <p>Add the workspace to the <code>workspaces</code> section of the TriggerTemplate.</p> <ol> <li> <p>Add the following code to the <code>workspaces</code> section</p> <pre><code>          - name: maven-settings\n            configMap:\n              name: maven-settings\n</code></pre> <p>Indentation Matters!</p> <p>Take care to match the indentation in the picture below</p> </li> <li> <p>Click <code>Save</code> to apply your changes</p> </li> </ol> <p></p> </li> </ol>"},{"location":"cloud-lab/devsecops/devsecops/#run-the-pipeline","title":"Run the pipeline","text":"<p>Go to the Actions menu of your pipeline and select Start.</p> <p></p> <p>Hit Start after reviewing the settings panel and making sure to set the options for the <code>source</code>(select <code>PersistentVolumeClaim</code> and your claim) and <code>maven-settings</code> ( select <code>configmap</code> as the resource choice and <code>maven-settings</code> as the specific configmap to use as in the image below) workspaces.</p> <p></p> <p>You can go to your pipeline logs and see the output for each of the tasks.  <p>It will take 15-20 minutes for the code analysis to run completely through. This task will wait until the quality check is complete in SonarQube and if the quality gate fails, this task will fail and the pipeline will not continue to run. If the quality gate succeeds, this task will succeed and progress onto the next task in the pipeline.</p> <p>Let's see if our code passes the code analysis...</p> <p></p> <p>It fails . Next, we are going to see why it failed.</p>"},{"location":"cloud-lab/devsecops/devsecops/#analyzing-the-failure-in-sonarqube","title":"Analyzing the Failure in SonarQube","text":""},{"location":"cloud-lab/devsecops/devsecops/#view-your-project","title":"View your project","text":"<p>At this point please return to the SonarQube server here, and view the code scan report to see what caused the quality check to fail. After logging in, please do the following:</p> <p></p> <ol> <li> <p>Type your name in the project search bar to bring up your project</p> </li> <li> <p>Click on your project (which should have a <code>Failed</code> label)</p> </li> </ol>"},{"location":"cloud-lab/devsecops/devsecops/#check-what-caused-the-failure","title":"Check what caused the failure","text":"<p>You can see that the overall code check failed due to a security rating worse than A. You should see 9 vulnerabilities that caused this failure. In order to check what these are, please click on the vulnerabilities link as shown in the image.</p> <p></p> <ol> <li> <p>See individual vulnerabilities and click on <code>Why is this an issue?</code></p> </li> <li> <p>Read the vulnerability descriptions to see why they are a problem and get insights into fixing them in the code.</p> </li> </ol>"},{"location":"cloud-lab/devsecops/devsecops/#update-petclinic-to-fix-the-issues-that-came-up-in-the-sonarqube-scan","title":"Update PetClinic to fix the issues that came up in the SonarQube scan","text":"<p>In the scan, there were various security issues related to the use of entity objects for data transfer instead of data transfer objects (DTOs) when using @RequestMapping and similar methods. In order to fix these, you will have to make changes to the java code for the application. Luckily for you, the changes have already been made on the <code>security-fixes</code> branch of the project. In order to bring these changes to the main branch you will need to make a pul request and merge the <code>security-fixes</code> branch into the main branch. </p> <p>You can do this with the following actions:</p> <ol> <li> <p>Go to your fork of the petclinic repository in GitHub and choose to create a new pull request</p> <p></p> <ol> <li> <p>Click on the <code>Pull Requests</code> tab</p> </li> <li> <p>Click on <code>New pull request</code></p> </li> </ol> </li> <li> <p>Change your base repository from the main repository to your fork.</p> <p></p> <ol> <li> <p>Click on base repository default of <code>ibm-wsc/spring-petclinic</code></p> </li> <li> <p>Change to your petclinic fork (in my case this is <code>siler23/petclinic</code> but yours will be different)</p> </li> </ol> </li> <li> <p>Choose the <code>security-fixes</code> branch to merge into the <code>main</code> branch and create your pull request</p> <p></p> <ol> <li>Choose <code>security-fixes</code> branch to compare to <code>main</code></li> <li>Click <code>Create pull request</code></li> </ol> </li> <li> <p>Write a justification for your pull request and confirm again that you want to create it</p> <p></p> <ol> <li> <p>Write a justification such as    <pre><code>Create fixes for all of the security vulnerabilities that showed up in the SonarQube scan.\n</code></pre></p> </li> <li> <p>Click <code>Create pull request</code></p> </li> </ol> </li> <li> <p>Merge your pull request, merging the <code>security-fixes</code> branch with all of the security fixes into the <code>main</code> branch.</p> <p></p> </li> <li> <p>Confirm the merge </p> <p></p> </li> <li> <p>Delete the <code>security-fixes</code> branch now that it's been successfully merged into the <code>main</code> branch of your petclinic repository fork.</p> <p></p> <ol> <li> <p>See that the <code>security-fixes</code> branch was successfully merged!</p> </li> <li> <p>Click <code>Delete branch</code> to delete the now superfluous <code>security-fixes</code> branch.</p> </li> </ol> </li> </ol>"},{"location":"cloud-lab/devsecops/devsecops/#verify-that-vulnerabilities-in-petclinic-have-been-patched","title":"Verify that vulnerabilities in petclinic have been patched","text":"<ol> <li> <p>See a new pipeline triggered back in the <code>Pipelines</code> view of your OpenShift namespace.</p> <p></p> </li> <li> <p>View the pipeline run and watch it successfully complete the <code>code-analysis</code> task.</p> <p></p> <p>Note</p> <p>You can also wait to see the other tasks pass but since the main goal of this section was to focus on integrating security into DevOps and you have already gone through the pipeline without the <code>code-analysis</code> task, there is really no need to do so.</p> </li> <li> <p>View the SonarQube server again to see the updated results for your project (based on the latest scan)</p> <ol> <li> <p>See your project passes and click on it for full results</p> <p>Tip</p> <p>Search for your project with your name like before.</p> <p></p> </li> <li> <p>View the final results of the scan.</p> <p></p> <p>Those pesky vulnerabilities have been squashed! </p> </li> </ol> </li> </ol>"},{"location":"cloud-lab/devsecops/devsecops/#summary","title":"Summary","text":"<p>In this section, you started on your DevSecOps journey by integrating SonarQube security scanning into your DevOps pipeline. Initially, the scan flagged several security vulnerabilities, causing the pipeline to fail before the vulnerable code could get packaged into a container. Next, you were able to dig into the vulnerabilities and figure out what needed to be changed with the SonarQube report. Then, you applied a security patch, eliminating the flagged security vulnerabilities in the PetClinic application. With these changes, your pipeline succeeded having containerized and deployed secure code. Finally, you are left with a pipeline set up to catch any new security vulnerabilities as soon as they appear. Congratulations!</p>"},{"location":"cloud-lab/devsecops/sonarqube/","title":"Setting up SonarQube server in OpenShift","text":""},{"location":"cloud-lab/devsecops/sonarqube/#sign-up-for-necessary-accounts","title":"Sign up for necessary accounts","text":"<ol> <li> <p>Get a 2nd OpenShift cluster trial (in addition to the one you got for the regular lab, using a different email) here.</p> <p>Note</p> <p>Within 48 hours after you register, be sure to \"Activate your account or entitlement\" following the instructions in the follow-on email sent to the email address you specified during registration.</p> </li> <li> <p>Setup access to the Container Images for IBM Z and LinuxONE using this link and scrolling down to the Getting Started section and then following it.</p> </li> </ol>"},{"location":"cloud-lab/devsecops/sonarqube/#access-openshift-cluster","title":"Access OpenShift Cluster","text":"<p>Note</p> <p>This should be a different OpenShift cluster than the one used for the main lab sections due to resource constraints.</p> <ol> <li> <p>Code for the project</p> <p>Please fork the code to your GitHub repository by clicking here. If this fails, you likely already have a forked version of the repository from the lab. If so, go to your fork and use that in the next step.</p> </li> <li> <p>Clone the git repository to your local computer </p> <p>a. Get the link from GitHub using the <code>Code</code> button on your forked repository and the <code>HTTPS</code> tab. </p> <p>b. Perform the clone locally in a terminal window using git clone + the link you copied such as <code>git clone https://github.com/siler23/spring-petclinic.git</code> for the example in <code>a.</code> above.</p> </li> <li> <p>Log into OpenShift in a terminal window locally.</p> <p>a. Click on your username in the upper right hand of the LinuxONE Community Cloud OpenShift UI.</p> <p>b. Click <code>Copy Login command</code></p> <p></p> <p>c. In the new window that opens click <code>Display Token</code> to generate a login token.</p> <p>Note</p> <p>You may be prompted to enter your LinuxONE Community Cloud username and password again.</p> <p></p> <p>d. Copy the login command.</p> <p></p> <p>e. Use the login command in your terminal to login to your OpenShift project.</p> <p></p> <p>Note</p> <p>Login token has been blurred in image for security purposes.</p> </li> </ol>"},{"location":"cloud-lab/devsecops/sonarqube/#create-openshift-resources-for-sonarqube-server","title":"Create OpenShift resources for SonarQube server","text":"<ol> <li> <p>Create a Kubernetes (OpenShift) secret for the IBM Z Container Registry using your API Key</p> <pre><code>oc create secret docker-registry z-container-registry --docker-username=iamapikey --docker-server='icr.io' --docker-password='YOUR_API_KEY'\n</code></pre> <p>Note</p> <p>Please replace <code>YOUR_API_KEY</code> with your API Key for the IBM Z container registry.</p> </li> <li> <p>Create OpenShift resources from PetClinic git repo you cloned.</p> <p>a. Change into the directory where you cloned your petclinic repo in step 2b of Access OpenShift Cluster.</p> <p>b. Apply the SonarQube server files to your project from the main directory of the cloned GitHub fork using the following command:</p> <pre><code>oc apply -f ocp-files/sonarqube-server\n</code></pre> <p>Example</p> <pre><code>deployment.apps/sonarqube created\nservice/sonarqube created\nroute.route.openshift.io/sonarqube created\npersistentvolumeclaim/sonarqube-data created\n</code></pre> </li> </ol>"},{"location":"cloud-lab/devsecops/sonarqube/#access-sonarqube-server","title":"Access SonarQube server","text":"<ol> <li> <p>Wait for the SonarQube server to come up and then access it at its route you can find the route via the oc command line tool in your logged in namespace using:</p> <pre><code>hostname=\"$(oc get route sonarqube -o jsonpath='{.spec.host}')\" &amp;&amp; echo \"https://${hostname}\"\n</code></pre> <p>Note</p> <p>You can also use the user interface (UI) of your project.</p> </li> <li> <p>Log into SonarQube server with default username/password of <code>admin</code>/<code>admin</code></p> </li> </ol>"},{"location":"cloud-lab/devsecops/sonarqube/#change-admin-password","title":"Change Admin password","text":"<ol> <li> <p>Access your account settings</p> <p></p> <ol> <li> <p>Click the <code>A</code> icon in the upper right hand corner</p> </li> <li> <p>Choose <code>My Account</code> from the droopdown menu</p> </li> </ol> </li> <li> <p>Change admin password </p> <p></p> <ol> <li> <p>Choose the <code>Security</code> tab at the top of the page</p> </li> <li> <p>Enter your old password of <code>admin</code> and choose + confirm a new password </p> </li> <li> <p>Click <code>Change Password</code></p> </li> </ol> </li> </ol>"},{"location":"cloud-lab/devsecops/sonarqube/#create-petclinic-default-sonarqube-quality-gate","title":"Create PetClinic Default SonarQube Quality Gate","text":"<ol> <li> <p>Go to the <code>Quality Gates</code> tab.</p> <p></p> </li> <li> <p>Copy BUILT-IN Sonar way quality gate</p> <ol> <li> <p>Choose to copy the gate      </p> </li> <li> <p>Choose a name for your new gate     </p> </li> </ol> </li> <li> <p>Add conditions for <code>Overall Code</code></p> <p></p> <ol> <li> <p>Click <code>Add Condition</code> for each Condition in the image above (the same ones used for <code>New Code</code>).</p> </li> <li> <p>Create each condition for <code>Overall Code</code> to match the image above.</p> </li> <li> <p>Click <code>Set as Default</code> to set the <code>PetClinic Quality Gate</code> as the default quality gate so that it will be used for the new projects created by users in the SonarQube section of the lab.</p> </li> </ol> </li> </ol>"},{"location":"cloud-lab/devsecops/sonarqube/#summary","title":"Summary","text":"<p>You have set up a SonarQube and are now ready to enjoy the SonarQube section of the lab using your own SonarQube server. Congratulations!!!</p> <p>Note</p> <p>Use the URL you used to access the SonarQube server as your SonarQube url in the SonarQube section.</p>"},{"location":"cloud-lab/full-dev-pipeline/","title":"Configure PetClinic's Integration and Deployment Pipeline to Meet Your Organization's Needs1","text":"<p>It's time to expand your pipeline to automate the integration and deployment process for your application (with specific configuration for your organization) using OpenShift Pipelines. This involves the following tasks:</p> <ol> <li> <p>Automate PetClinic Build and Test for your Organization's Needs</p> <ul> <li>Automate MySQL deployment using OpenShift template</li> <li>Make clean container image from S2I build to meet the security needs of your organization</li> </ul> </li> <li> <p>Automate PetClinic development deployment to meet your Organization's Needs1</p> <ul> <li>Manage PetClinic deployment resources using KUSTOMIZE</li> <li>Setup PetClinic container image deployment automation with tagging based on source</li> </ul> </li> </ol> <ol> <li> <p>For the purposes of this lab, you are fulfilling the requirements of a fictional organization. These requirements could change for your specific organization but would follow a similar pattern with different specifics.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"cloud-lab/full-dev-pipeline/pipeline/","title":"Configure PetClinic Build and Test to Meet your Organization's Requirements1","text":"<p>Now that PetClinic is up and running on your OpenShift cluster, it's time to add functionality to your pipeline to achieve basic integration and deployment when triggered. The OpenShift pipeline you created in the PetClinic Up and Running uses Tekton to run a series of tasks (each with one or more steps) to accomplish a workflow (pipeline). You will use the Pipeline Builder UI built into OpenShift to quickly and easily craft a pipeline that meets your specific needs.</p> <p>Why OpenShift Pipelines?</p> <ul> <li> <p>Portable: OpenShift resources defined via yaml files -&gt; portable across OpenShift clusters</p> </li> <li> <p>Low Resource Usage: Containers spin up when triggered -&gt; resources only used when needed</p> </li> <li> <p>Configurable: Can tailor overall pipeline and individual tasks to needs of your enterprise/organization </p> </li> <li> <p>Ease of Use: Pipeline Builder UI and built-in cluster resources (i.e. <code>ClusterTasks</code>, <code>ClusterTriggerBindings</code>, etc.) enable you to easily create a pipeline and export the yaml files with minimal knowledge</p> </li> </ul>"},{"location":"cloud-lab/full-dev-pipeline/pipeline/#petclinic-pipeline","title":"PetClinic Pipeline","text":"<p>When you deployed the PetClinic application using the <code>From Git</code> option in the PetClinic Up and Running section, you chose to create a basic pipeline. You'll start with this pipeline and edit it to add new functionality for your use case. </p> <p>Navigate to the <code>Pipelines</code> tab in the <code>Developer</code> perspective on the left and then click the three dots to the right of the pipeline name (<code>spring-petclinic</code>) and choose <code>Edit Pipeline</code>.  </p>"},{"location":"cloud-lab/full-dev-pipeline/pipeline/#ensure-mysql-database-deployed-for-each-run","title":"Ensure MySQL Database Deployed for each Run","text":"<p>This will bring you to the Pipeline Builder UI where you can edit your pipeline. Here you will make sure the MySQL database is configured according to your specification before the <code>build</code> task.</p> <ol> <li> <p>Add a <code>mysql-deploy</code> task in parallel to the <code>git-fetch</code> task.       </p> <p>Why is <code>mysql-deploy</code> in Parallel?</p> <p>This ensures MySQL is in place for each <code>PetClinic</code> application build (which would fail without it).  </p> <p>Click <code>Select Task</code> in the middle of the rectangle of the new task and choose the <code>openshift-client</code> task from the dropdown menu. </p> <p></p> <p>Click on the middle of the oval of the <code>openshift-client</code> task to enter values for it (copy and paste boxes below image).</p> <p></p> <p>Tip</p> <p>Once you add a specific task (i.e. <code>openshift-client</code>), clicking on the oval of the task will enable you to edit its default values for your needs.</p> <p>Give the task the following parameters to ensure the MySQL database is available with the necessary configuration:</p> <p></p> Display Name<pre><code>mysql-deploy\n</code></pre> SCRIPT<pre><code>oc process openshift//mysql-ephemeral -p MYSQL_USER=petclinic -p MYSQL_PASSWORD=petclinic -p MYSQL_ROOT_PASSWORD=petclinic -p MYSQL_DATABASE=petclinic | oc apply -f -\n</code></pre> <p>and an empty <code>ARGS</code> value.</p> <p>No help please!</p> <p>Make sure <code>help</code> is deleted from the <code>ARGS</code> section (click the - button to delete the default help args line).</p> <p>Simply Click Away</p> <p>Once you have entered the string into the <code>SCRIPT</code> section and deleted the help arg, just click away (i.e. on a regular section of the page) to get the configuration menu to go away and keep the new value(s) you just entered for the task.</p> <p>What is <code>oc process</code> doing?</p> <p><code>oc process</code> is processing the OpenShift template for the <code>mysql-ephemeral</code> database with the parameters given via a series of <code>-p</code> arguments and finally <code>oc apply -f -</code> ensures that any missing components will be recreated.</p> </li> <li> <p>Add a <code>mysql-rollout-wait</code> task</p> <p>You need to make sure that <code>mysql</code> is fully deployed before your <code>build</code> task begins. In order to achieve this, you will use the OpenShift Client again and wait for the <code>rollout</code> of the <code>mysql</code> <code>deploymentConfig</code> to complete after the <code>mysql-deploy</code> task. Add a sequential task after <code>mysql-deploy</code>:</p> <p></p> <p><code>Select Task</code> as <code>openshift-client</code> like before and then fill out the task with the following parameters (copy and paste boxes below image for changes):</p> <p></p> Display Name<pre><code>mysql-rollout-wait\n</code></pre> <p>ARGS</p> Arg 1<pre><code>rollout\n</code></pre> Arg 2<pre><code>status\n</code></pre> Arg 3<pre><code>dc/mysql\n</code></pre> <p>What the ARGS?</p> <p>You may be wondering why you used the <code>SCRIPT</code> section in the <code>mysql-deploy</code> task for the entire command, but now are using the <code>ARGS</code> to individually list each argument of the command? Both work and so you are going through both methods here. On the one hand, the <code>SCRIPT</code> method is easier to copy and paste and looks the same as it would entered on the command line. On the other hand, the <code>ARGS</code> method adds readability to the task. Choose whichever method you prefer, though beware of input errors  with the <code>ARGS</code> method for long commands. FYI: The equivalent <code>SCRIPT</code> command for the <code>mysql-rollout-wait</code> task is:</p> <pre><code>oc rollout status dc/mysql\n</code></pre> </li> </ol> <p> Now your <code>mysql-deploy</code> and <code>mysql-rollout</code> tasks will have <code>MySQL</code> alive and well for the <code>build</code> task!</p>"},{"location":"cloud-lab/full-dev-pipeline/pipeline/#make-clean-image-from-s2i-build","title":"Make Clean Image from S2I build","text":"<p>The <code>s2i-java-11</code> container image is very convenient for making a container image from source code. However, the simplicity that gives it value can make it fail at meeting the needs of many organizations by itself. In your case, you will take the artifacts from the s2i container image and copy them to a new container image that can meet all your needs to get the best of both worlds. You'll create an optimized container image starting from a compact <code>openj9</code> java 11 base and employing the advanced layers feature in spring that optimizes Docker image caching with the final-Dockerfile in the ibm-wsc/spring-petclinic git repository you forked. </p> <ol> <li> <p>Add <code>Buildah</code> task</p> <p>Add the <code>buildah</code> task as a sequential task after the <code>build</code> task.</p> <p></p> </li> <li> <p>Configure <code>buildah</code> task</p> <p>Tip</p> <p>Each value that you need to configure is listed below with the value in a click-to-copy window (other values can be left alone to match the image)</p> <p></p> Display Name<pre><code>clean-image\n</code></pre> IMAGE<pre><code>$(params.IMAGE_NAME)-minimal:$(params.COMMIT_SHA)\n</code></pre> DOCKERFILE<pre><code>./final-Dockerfile\n</code></pre> TLSVERIFY<pre><code>false\n</code></pre> BUILD_EXTRA_ARGS<pre><code>--build-arg PETCLINIC_S2I_IMAGE=$(params.IMAGE_NAME)\n</code></pre> SOURCE (choose from dropdown)<pre><code>workspace\n</code></pre> </li> <li> <p>Add <code>GIT_MESSAGE</code>, and <code>COMMIT_SHA</code> parameters to the pipeline</p> <p>Click <code>Add Parameter</code> twice ...</p> <p></p> <p>and then fill in the parameter details for <code>GIT_MESSAGE</code> and <code>COMMIT_SHA</code> (copy and paste boxes below image)</p> <p></p> <p>GIT_MESSAGE</p> Parameter Name<pre><code>GIT_MESSAGE\n</code></pre> Parameter Description<pre><code>Git commit message if triggered by Git, otherwise it's a manual build\n</code></pre> Parameter Default Value<pre><code>This is a manual build (not triggered by Git)\n</code></pre> <p>COMMIT_SHA</p> Parameter Name<pre><code>COMMIT_SHA\n</code></pre> Parameter Description<pre><code>SHA of Git commit if triggered by Git, otherwise just update manual tag\n</code></pre> Parameter Default Value<pre><code>manual\n</code></pre> <p>Tip</p> <p>Save the parameters when you are done with entry by clicking on blue <code>Save</code> box before moving onto step 4. If blue <code>Save</code> box doesn't appear (is greyed out) delete extra blank parameters you may have accidentally added with the <code>-</code>.</p> </li> </ol>"},{"location":"cloud-lab/full-dev-pipeline/pipeline/#summary","title":"Summary","text":"<p>Your pipeline will now automatically check that your <code>MySQL</code> instance is configured properly and rolled out before moving on to the build stage (instead of doing this as a manual task like in the previous section of the lab). Moreover, it will curate the final PetClinic (<code>minimal</code>) container image to only have the necessary components instead of a bunch of extra packages (required only for the build itself) that add bloat and potential security vulnerabilities to your container image. Finally, it will tag the container image to distinguish between manual builds and those triggered by a potential git push. In the next section, you will see this automation in action for your development environment.</p> <ol> <li> <p>For the purposes of this lab, you are fulfilling the requirements of a fictional organization. These requirements could change for your specific organization but would follow a similar pattern with different specifics.\u00a0\u21a9</p> </li> </ol>"},{"location":"cloud-lab/full-dev-pipeline/runpipeline/","title":"Configure PetClinic Development Deployment to Meet your Organization's Requirements1","text":""},{"location":"cloud-lab/full-dev-pipeline/runpipeline/#manage-resource-across-environments-with-kustomize","title":"Manage resource across environments with Kustomize","text":"<p>Kustomize is a tool for customizing Kubernetes resource configuration.</p> <p>From the documentation overview</p> <p>Kustomize traverses a Kubernetes manifest to add, remove or update configuration options without forking. It is available both as a standalone binary and as a native feature of kubectl. See the Introducing Kustomize Kubernetes Blog Post for a more in-depth overview of Kustomize and its purpose.</p> <p>As part of doing things the \"cloud native\" way you will be using Kustomize to manage resource changes across your <code>dev</code> and <code>staging</code> environments as well as injecting information from your pipeline (such as newly created container image information with git commits) into your Kubernetes (OpenShift) resources. </p> <p>To see how you use Kustomize, see the Kustomize configuration in your GitHub code in the subdirectories of the ocp-files directory.</p> <p>For more information on how kubectl (and oc through kubectl) integrates Kustomize, see the kubectl documentation.</p>"},{"location":"cloud-lab/full-dev-pipeline/runpipeline/#creating-custom-task-for-kustomize","title":"Creating Custom Task for Kustomize","text":"<p>Since there is no ClusterTask defined for Kustomize, you will create a custom task for this purpose. It will change into the Kustomize directory, run a Kustomize command on the directory, and then apply the files from the directory using the built-in Kustomize functionality of the oc command line tool (via kubectl's Kustomize support)</p> <ol> <li> <p>Copy the <code>kustomize</code> task using the following definition (copy by clicking on the copy icon in the top right of the box below):</p> <pre><code>apiVersion: tekton.dev/v1beta1\nkind: Task\nmetadata:\nname: kustomize\nspec:\ndescription: &gt;-\nThis task runs commands against the cluster where the task run is being\nexecuted.\n\nKustomize is a tool for Kubernetes native configuration management.  It\nintroduces a template-free way to customize application configuration that\nsimplifies the use of off-the-shelf applications.  Now, built into kubectl\nas apply -k and oc as oc apply -k.\nparams:\n- default: ocp-files\ndescription: The directory where the kustomization yaml file(s) reside in the git directory\nname: KUSTOMIZE_DIR\ntype: string\n- default: base\ndescription: subdirectory of KUSTOMIZE_DIR used for extra configuration of current resources\nname: EDIT_SUDBDIR\ntype: string\n- default: overlay/dev\ndescription: subdirectory of KUSTOMIZE_DIR used for specifying resources for a specific release such as dev or staging\nname: RELEASE_SUBDIR\ntype: string\n- default: kustomize --help\ndescription: The Kustomize CLI command to run\nname: SCRIPT\ntype: string\nsteps:\n- image: 'quay.io/gmoney23/kustomize-s390x:v4.1.2'\nname: kustomize\nresources:\nlimits:\ncpu: 200m\nmemory: 200Mi\nrequests:\ncpu: 200m\nmemory: 200Mi\nworkingDir: \"$(workspaces.source.path)/$(params.KUSTOMIZE_DIR)/$(params.EDIT_SUDBDIR)\"\nscript: $(params.SCRIPT)\n- image: 'image-registry.openshift-image-registry.svc:5000/openshift/cli:latest'\nname: apply-oc-files\nresources:\nlimits:\ncpu: 200m\nmemory: 200Mi\nrequests:\ncpu: 200m\nmemory: 200Mi\nscript: oc apply -k \"$(workspaces.source.path)/$(params.KUSTOMIZE_DIR)/$(params.RELEASE_SUBDIR)\"\nworkspaces:\n- name: source\ndescription: The git source code\n</code></pre> </li> <li> <p>Create the <code>kustomize</code> Task</p> <p>a. Click <code>Import YAML</code> to bring up the box where you can create Kubernetes resource definitions from yaml</p> <p>b. Paste the <code>kustomize</code> Task into the box</p> <p>c. Scroll down and click <code>Create</code> to create the <code>kustomize</code> Task </p> <p></p> </li> </ol> <p>You should now see the created <code>kustomize</code> Task.</p> <p></p> <p>Finally, navigate back to the <code>Pipelines</code> section of the OpenShift UI and go back to editing your pipeline.</p>"},{"location":"cloud-lab/full-dev-pipeline/runpipeline/#add-kustomize-task-to-pipeline","title":"Add Kustomize Task to Pipeline","text":"<ol> <li> <p>Add a sequential task after <code>clean-image</code> and when you <code>Select Task</code> choose the <code>kustomize</code> task. </p> <p></p> </li> <li> <p>Configure <code>kustomize</code> task</p> <p>Since your initial deploy will be for the <code>dev</code> environment, the only values you need to change are the <code>Display Name</code> and the <code>SCRIPT</code> (copy and paste boxes below image):</p> <p></p> Display Name<pre><code>kustomize-dev\n</code></pre> SCRIPT<pre><code>kustomize edit set image spring-petclinic=$(params.IMAGE_NAME)-minimal:$(params.COMMIT_SHA)\n</code></pre> SOURCE (choose from dropdown)<pre><code>workspace\n</code></pre> </li> <li> <p><code>Save</code> the pipeline</p> </li> </ol>"},{"location":"cloud-lab/full-dev-pipeline/runpipeline/#clean-old-petclinic-instances-at-the-beginning-of-a-run","title":"Clean Old PetClinic Instances at the Beginning of a Run","text":"<ol> <li> <p>Go back to editing your pipeline via <code>Actions -&gt; Edit Pipeline</code></p> <p></p> </li> <li> <p>Add a Task named <code>cleanup-resources</code> sequentially at the beginning of the pipeline before <code>fetch-repository</code> (using the <code>openshift-client</code> ClusterTask).</p> <p></p> </li> <li> <p>Configure the task with the following parameters (copy and paste boxes below image for changes):</p> <p></p> Display Name<pre><code>cleanup-resources\n</code></pre> SCRIPT<pre><code>oc delete deployment,cm,svc,route -l app=$(params.APP_NAME) --ignore-not-found\n</code></pre> <p>and an empty <code>ARGS</code> value.</p> <p>No help please!</p> <p>Make sure <code>help</code> is deleted from the <code>ARGS</code> section (click the - button to delete the default help args line).</p> </li> </ol>"},{"location":"cloud-lab/full-dev-pipeline/runpipeline/#update-deploy-task-to-deploy-dev","title":"Update Deploy Task to deploy-dev","text":"<ol> <li> <p>Click on the <code>deploy</code> Task at the end of the pipeline and change the following parameters to the corresponding values (copy and paste boxes below image):</p> <p></p> Display Name<pre><code>deploy-dev\n</code></pre> SCRIPT<pre><code>echo \"$(params.GIT_MESSAGE)\" &amp;&amp; oc $@\n</code></pre> New Last Arg<pre><code>deploy/spring-petclinic-dev\n</code></pre> </li> <li> <p><code>Save</code> your pipeline!</p> </li> </ol>"},{"location":"cloud-lab/full-dev-pipeline/runpipeline/#run-the-updated-pipeline","title":"Run the Updated Pipeline","text":"<ol> <li> <p>Go to <code>Actions</code> -&gt; <code>Start</code> in the right hand corner of the pipeline menu</p> <p></p> </li> <li> <p>Manually trigger a <code>PipelineRun</code> by accepting the default values and clicking on <code>Start</code>.</p> <p>Persistent Volume Claim Note</p> <p>Please select a <code>PersistentVolumeClaim</code> if it is not already filled out for you to complete your pipeline. If it is already filled out for you then jump right to starting the pipeline.</p> <p></p> </li> <li> <p>Watch the results of your build pipeline run. It should complete successfully as in the pictures below.</p> <p>How long will your pipeline take to run?</p> <p>The pipeline run may take anywhere from 10-25 minutes to complete depending on the current system load. You can see the progress of your build, as well as if any errors occur, via the UI. Thus, by monitoring the UI, you can make sure things are going as planned.</p> <p>Pipeline Run Success View Perspective:</p> <p></p> <p>Pipeline Run Details View</p> <p>In the pipeline run <code>Details</code> view, you can see the pipeline run succeeded with all tasks having a green check mark. Additionally, the pipeline run in the above screenshot was <code>Triggered By</code> a user versus an automated source such as an event listener watching for a GitHub push...</p> <p>Pipeline Run Success Logs Perspective:</p> <p></p> <p>Pipeline Run Logs View</p> <p>From the pipeline run <code>Logs</code> view you can see that the pipeline run tasks all have green check marks and that this was a manual build (from the message in the log output of the final [<code>deploy-dev</code>] task).</p> </li> </ol>"},{"location":"cloud-lab/full-dev-pipeline/runpipeline/#summary","title":"Summary","text":"<p>Congratulations! You successfully deployed your PetClinic application to your development environment with automated checks and configuration to meet your needs. This means that whenever your pipeline is triggered it will automatically spin up resources to build, test and deploy your application according to the specification you need to meet for your organization1.</p> <ol> <li> <p>For the purposes of this lab, you are fulfilling the requirements of a fictional organization. These requirements could change for your specific organization but would follow a similar pattern with different specifics.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"on-premises-lab/application-promotion/","title":"It's Time to Get your Pet Clinic Ready for its Internal Debut","text":"<p>In this section, you will bring PetClinic from development to staging for the internal showcase of your pet clinic (staging).</p> <ol> <li> <p>Promotion Tasks</p> <ul> <li>Check successful connection of PetClinic <code>dev</code> version</li> <li>Deploy PetClinic <code>staging</code> version</li> <li>Check successful connection of PetClinic <code>staging</code> version</li> </ul> </li> <li> <p>Git Tasks</p> <ul> <li>Add Gogs trigger to pipeline</li> <li>Pass git commit messages and hashes to pipeline</li> <li>Version images by git commit</li> <li>Add webhook to Gogs so it will trigger a new <code>PipelineRun</code> for each push</li> </ul> </li> <li> <p>Running Pipeline</p> <ul> <li>Update PetClinic with new animal type and push to Gogs</li> <li>Watch Gogs trigger <code>PipelineRun</code></li> <li>Watch app move from dev to staging seamlessly with images tagged with git commit SHA </li> <li>Interact with the PetClinic application in staging using the new animal type</li> </ul> </li> </ol>"},{"location":"on-premises-lab/application-promotion/action/","title":"CI/CD in Action","text":""},{"location":"on-premises-lab/application-promotion/action/#make-a-change-in-gogs","title":"Make a change in Gogs","text":"<ol> <li> <p>Navigate to your <code>spring-petclinic</code> repository's main page (this is the <code>Files</code> tab if you are on a different tab such as settings)</p> </li> <li> <p>Click on the following sections in order to navigate to the correct file</p> <ol> <li>src</li> <li>main</li> <li>resources</li> <li>db</li> <li>mysql</li> <li>data.sql</li> </ol> </li> <li> <p>Edit the file by clicking on the pencil icon</p> <p></p> </li> <li> <p>Change the file to add the pet field of your choice and commit it to your Gogs repository (description and copy and paste box are below image)</p> <p></p> <ol> <li> <p>Make a change using the pet type you want to add (example is a turtle) </p> <p>Note</p> <p>The copy and paste box below can be entered on line 24 with <code>enter</code> pressed after it to match the image above.</p> <p>Turtle</p> <pre><code>INSERT IGNORE INTO types VALUES (7, 'turtle');\n</code></pre> <p>I want to add Willow, an awesome armadillo, not Yertle the turtle!</p> <p>If you want to add something other than a turtle as an option, please change <code>turtle</code> to that animal (i.e. <code>armadillo</code>) in the mysql statement above. For the armadillo example, the statement becomes:</p> <pre><code>INSERT IGNORE INTO types VALUES (7, 'armadillo');\n</code></pre> </li> <li> <p>Type in a commit message (you can make this whatever you want) and commit the change (example copy and paste boxes from image above)</p> <p>Yertle the turtle</p> Title<pre><code>Turtle Time\n</code></pre> Extended Description<pre><code>I want to be able to add Yertle the turtle.\n</code></pre> </li> </ol> </li> <li> <p>Take note of the git commit message and hash</p> <ol> <li> <p>Click on the <code>Files</code> tab to go back to the main repository page</p> </li> <li> <p>Look at the git commit hash (id)</p> <p></p> </li> </ol> </li> </ol>"},{"location":"on-premises-lab/application-promotion/action/#continuous-integration-via-openshift-pipelines","title":"Continuous Integration via OpenShift Pipelines","text":""},{"location":"on-premises-lab/application-promotion/action/#successfully-run-pipeline-via-gogs","title":"Successfully Run Pipeline via Gogs","text":"<ol> <li> <p>Visit the newly triggered pipeline run in the <code>Pipelines</code> menu in the OpenShift UI</p> <p></p> </li> <li> <p>View the pipeline run from the <code>Details</code> view</p> <p></p> <p>You can see your user has triggered the <code>PipelineRun</code> via your event listener.</p> </li> <li> <p>You can see the variables populated with the correct values from Gogs in the <code>YAML</code> view of the pipeline run.</p> <p></p> </li> <li> <p>Watch the results of your build pipeline run. It should complete successfully as in the pictures below. </p> <p>Your pipeline may take a while to run</p> <p>The pipeline run may take anywhere from 10-25 minutes to complete depending on the current system load. You can see the progress of your build, as well as if any errors occur, via the UI. Thus, by monitoring the UI, you can make sure things are going as planned.</p> <p>What should you do if your pipeline run ends in failure</p> <p>If your pipeline run ends in failure, please look at the <code>Failure</code> tab (immediately below this message) to get back on track (instead of the default <code>Success</code> tab).</p> SuccessFailure <p>Pipeline Run Success View Perspective:</p> <p></p> <p>Pipeline Run Details View</p> <p>In the pipeline run <code>Details</code> view, you can see the pipeline run succeeded with all tasks having a green check mark. Additionally, observe that the event listener has triggered the <code>PipelineRun</code> instead of a user this time.</p> <p>Pipeline Run Success Logs Perspective:</p> <p></p> <p>Pipeline Run Logs View 1</p> <p>In the pipeline run <code>Logs</code> view, you can also see that the pipeline run tasks all have green check marks. Looking at the last task, you can see that the that the external connection check worked and the PetClinic application is available at the route printed in the logs. Additionally, you can see via the series of tasks marked with green checks that the dev deployment ran successfully and the system cleaned it up and ran the staging deployment successfully to complete the pipeline.</p> <p></p> <p>Pipeline Run Logs View 2</p> <p>When you switch to the <code>deploy-staging</code> task logs, by clicking on the <code>task</code> on the left hand side of the <code>Logs</code> view of the pipeline run, you see this was an automated build from git since the task prints out the <code>GIT_MESSAGE</code> that you typed in your commit word for word. (Note: If you chose a different commit message that will show instead of the one displayed in the image above.).</p> <p>Your pipeline failed, here is how to get back on the happy path</p> <ol> <li> <p>Please review your <code>pipelineRun</code> and see what error caused the failure.</p> </li> <li> <p>Make changes to fix the error. (If it's unclear what is causing the error / how to fix it, please ask the instructors for help) </p> </li> <li> <p>Resend the webhook from Gogs to trigger a new <code>pipelineRun</code> with the same values as before (see images below for help)</p> <ol> <li> <p>Click on your webhook from the <code>Webhooks</code> section of the repository settings for your Gogs repository fork of the <code>spring-petclinic</code> repository</p> <p></p> </li> <li> <p>Scroll down to <code>Recent Deliveries</code></p> </li> <li>Click on the most recent delivery</li> <li> <p>Click <code>Redelivery</code></p> <p></p> </li> </ol> </li> </ol> </li> </ol>"},{"location":"on-premises-lab/application-promotion/action/#see-changes-in-application","title":"See Changes in Application","text":"<ol> <li> <p>Navigate to the <code>Topology</code> view and open a new tab with your recently deployed <code>staging</code> version of the PetClinic application by clicking <code>Open URL</code>.</p> <p></p> </li> <li> <p>Navigate to the <code>Find Owners</code> tab</p> <p></p> </li> <li> <p>Choose to add a new owner</p> <p></p> </li> <li> <p>Add the owner with details of your choice</p> <p></p> </li> <li> <p>Choose to add one of the owner's pets</p> <p></p> </li> <li> <p>Fill in the pet's details and select the new type of pet you added (turtle for the example)</p> <p></p> </li> <li> <p>View the newly created pet of the new type (Yertle the turtle for the example)</p> <p></p> </li> </ol>"},{"location":"on-premises-lab/application-promotion/action/#summary","title":"Summary","text":"<p>In this section, you made a change to your PetClinic application to add a new pet type of your choice and pushed the change to Gogs. This triggered a new pipeline run which built a new image for the application tagged with the git commit hash and displayed the commit message explaining the change the build was implementing. Next, your pipeline deployed this change to OpenShift in development, tested it internally and externally and then rolled it out to staging (where it was also tested automatically). Finally, you visited the application and used the new feature (new type of pet) by adding a pet of that type to a new owner successfully. In other words, you are off the ground and running with \"cloud native\" CI/CD for your PetClinic application on IBM Z/LinuxONE! Congratulations!!!</p>"},{"location":"on-premises-lab/application-promotion/git/","title":"Integrating OpenShift Pipelines with Gogs","text":"<p>It's time to add the <code>C</code> (continuous) to your CI/CD pipeline.</p>"},{"location":"on-premises-lab/application-promotion/git/#add-a-gogs-trigger","title":"Add a Gogs Trigger","text":"<ol> <li> <p>Choose <code>Add Trigger</code> from the pipeline menu</p> <p></p> </li> <li> <p>Configure the trigger as follows (copy and paste boxes below image) and click <code>Add</code> to add the trigger to your pipeline:</p> <p></p> <p>Note</p> <p>The <code>Git_Repo</code> parameter should have your student instead of <code>student00</code>. This should already be correctly filled out for you, so please don't change that to <code>student00</code>.</p> Git Provider Type<pre><code>gogs-push\n</code></pre> <p>Note</p> <p><code>gogs-push</code> is in a menu you need to select from</p> GIT_MESSAGE<pre><code>$(tt.params.git-commit-message)\n</code></pre> COMMIT_SHA<pre><code>$(tt.params.git-commit-id)\n</code></pre> </li> </ol> <p>You are choosing the <code>gogs-push</code> cluster trigger binding, which we defined for our cluster using the webhook parameters from Gogs. This passes information into a number of different variables which you can list by clicking the expand arrow seen in the picture (It will initially say <code>Show Variables</code> and then switch to <code>Hide Variables</code> when expanded as shown in the picture). You will be using the variables in green boxes in the picture to pass the git commit message (<code>git-commit-message</code>) as well as the SHA (id) of the git commit (<code>git-commit-id</code>) to the build pipeline from the Gogs webhook that triggers the build.</p>"},{"location":"on-premises-lab/application-promotion/git/#setting-up-git-webhook","title":"Setting up Git Webhook","text":"<p>Now, you need to set up a webhook from Gogs. You want this to hit your <code>event listener</code>, the pipelines resource which listens for events from outside sources in order to trigger a build. The listener you set up is using the <code>gogs-push</code> trigger binding to trigger a new pipeline run for your <code>spring-petclinic</code> pipeline passing the <code>gogs-push</code> parameters mentioned before. You created this <code>event-listener</code> via the OpenShift Pipelines UI when you added a trigger and will see it in the <code>Topology</code> section of the OpenShift UI as another application when you travel back there later. In order to setup your webhook to send a message to the <code>event listener</code> after a git push, do the following:</p> <ol> <li> <p>Find the event listener url from the <code>Details</code> view of your pipeline and copy it using the copy button</p> <p></p> <p>Find the value listed for your pipeline and copy that value.</p> </li> <li> <p>Navigate to your git repository of the <code>spring-petclinic</code> application.</p> <p>Tip</p> <p>Your git repository should be in the form <code>https://gogs-ui-gogs.apps.atsocpd3.dmz/</code>yourstudent<code>/spring-petclinic</code> where yourstudent is your lab student such as <code>https://gogs-ui-gogs.apps.atsocpd3.dmz/student00/spring-petclinic</code> for student00. You can also navigate to your repository in gogs here by logging in (if not logged in already) and then clicking on the <code>spring-petclinic</code> repository from your repositories like you did in the first section of the lab.</p> </li> <li> <p>Go to the <code>settings</code> page of the repository and add a <code>Gogs</code> webhook</p> <p></p> </li> <li> <p>Go to the Webhooks section and add a webhook with: </p> <ol> <li> <p><code>event listener URL</code> as the <code>PAYLOAD_URL</code></p> </li> <li> <p><code>application/json</code> selected as the <code>Content type</code></p> </li> <li> <p><code>Just the push event</code> selected for <code>Which events would you like to trigger this webhook?</code>.</p> </li> </ol> <p></p> </li> <li> <p>See the successfully created webhook now listed</p> <p></p> </li> </ol>"},{"location":"on-premises-lab/application-promotion/git/#summary","title":"Summary","text":"<p>You created a Gogs webhook for your <code>spring-petclinic</code> repository that will trigger a new run of your <code>spring-petclinic</code> pipeline when new code is pushed to your Gogs repo1. You will trigger your pipeline via Gogs in the next section.</p> <ol> <li> <p>A more detailed explanation is that when new code is pushed to your Gogs repo, the Gogs webhook will send a payload to the event listener which then interacts with a number of OpenShift Pipelines-associated Kubernetes custom resources that you created when you used the <code>Add Trigger</code> button in the UI. Namely, the event listener will trigger a new <code>PipelineRun</code> of your <code>spring-petclinic</code> pipeline based on the <code>spring-petclinic</code> <code>TriggerTemplate</code> passing it the values for the git commit SHA hash and the commit message using the variables populated via the <code>gogs-push</code> <code>ClusterTriggerBinding</code>.\u00a0\u21a9</p> </li> </ol>"},{"location":"on-premises-lab/application-promotion/promote/","title":"Automatically Testing and Promoting your Application","text":"<p>Here you will edit your pipeline to test your application in development, clean up your development resources, promote your application to staging, and test it in staging.</p>"},{"location":"on-premises-lab/application-promotion/promote/#testing-your-application-in-the-wild","title":"Testing your Application in the Wild","text":"<p>During the build stage of your pipeline, you tested two things:</p> <ol> <li>That the pieces of your application worked (unit testing)</li> <li>That they worked together (integration testing)</li> </ol> <p>Now, it's time to go a step further and automate testing that your application is working and accessible when deployed in a real (OpenShift) environment:</p> <ol> <li>Internally (within OpenShift)</li> <li>Externally (the outside world)</li> </ol>"},{"location":"on-premises-lab/application-promotion/promote/#internally-within-kubernetesopenshift","title":"Internally (Within Kubernetes/OpenShift)","text":"<p>The first thing you need to test is that the application is alive and available from within your cluster (Kubernetes environment). This is important not only for when running the CI/CD pipeline, but also for any time your application is running  (downtime is detrimental, especially in production). </p> <p>This functionality is available in Kubernetes via probes. There are 3 different types of probes to test the different aspects of your application's availability:</p> <p>Kubernetes Probes in Spring</p> <p>In Spring there are built-in endpoints for Kubernetes probes. If you are interested in learning how to program these into a Spring application of yours in the future, please take a look at Spring's official blog.</p> <ol> <li> <p>Startup probes:</p> <ol> <li> <p>Activate first</p> </li> <li> <p>Make sure an application is up and running (started up) </p> </li> <li> <p>Free startup concerns/constraints from other probes</p> </li> </ol> <p>Here is the <code>startupProbe</code> for the container running the PetClinic application:</p> <pre><code>startupProbe:\nhttpGet:\npath: /actuator/health/liveness\nport: 8080\nperiodSeconds: 10\nfailureThreshold: 30\n</code></pre> <p>It simply queries (via localhost) PetClinic's liveness health endpoint. Once this returns successfully, you can be confident the application has started up and can begin to monitor the liveness and readiness of each container of each replica (pod) of your application throughout its lifecycle.</p> </li> <li> <p>Liveness probes:</p> <ol> <li> <p>Make sure an application is actually running and not caught in a deadlock (it's alive)</p> </li> <li> <p>Restart \"dead\" containers automatically with kubelet</p> </li> <li> <p>Fix problems that may arise in long-running containers via the aforementioned restart</p> </li> </ol> <p>Here is the <code>livenessProbe</code> for the container running the PetClinic application:</p> <pre><code>livenessProbe:\nhttpGet:\npath: /actuator/health/liveness\nport: 8080\nperiodSeconds: 10\nfailureThreshold: 3\n</code></pre> <p>This looks almost identical to the <code>startupProbe</code> above other than having a much lower <code>failureThreshold</code>. The <code>startupProbe</code> is making sure the container of a given pod of your application's deployment is alive when it first starts up (It is allowing time for that startup to occur). On the other hand, the <code>liveness</code> probe above is making sure your application stays alive throughout its lifecycle. Therefore, it has a much lower <code>failureThreshold</code> to enable kubelet to quickly respond (restart the container) when the container becomes deadlocked.</p> </li> <li> <p>Readiness probes:</p> <ol> <li> <p>Check if each copy (replica) of an application is ready</p> </li> <li> <p>Makes sure traffic goes only to replicas that are ready for it</p> </li> <li> <p>Prevents users from interacting with unready replicas (getting unnecessary errors)</p> </li> </ol> <p>Here is the <code>readinessProbe</code> for the container running PetClinic:</p> <pre><code>readinessProbe:\nhttpGet:\npath: /actuator/health/readiness\nport: 8080\nperiodSeconds: 10\n</code></pre> <p>It simply queries (via localhost) PetClinic's readiness health endpoint. This probe will let Kubernetes know when to send traffic to a PetClinic replica. When you send traffic to the application, only the available replicas will receive it. This means that replicas which aren't ready for traffic don't accidentally get it, preventing errors for the user.</p> </li> </ol> <p>These 3 probes serve to declare to Kubernetes the way your application (and the replicas that make it up) should behave, enabling the system to monitor and take action on your behalf (restarting the container or removing its pod's endpoint from service) when the current state (the status) does not meet the desired state (your specification).</p> <p>The rollout task you created before as <code>deploy-dev</code> will only complete once all desired replicas are ready, implying that both the <code>startup</code> (initial liveness) and <code>readiness</code> probes have successfully passed and all replicas of your application are initially alive and ready for business. </p>"},{"location":"on-premises-lab/application-promotion/promote/#testing-external-connections","title":"Testing External Connections","text":"<p>While making sure your application is internally up and running is important, at the end of the day you want to provide access to your users externally1. </p> <p>This means it is important to also test the OpenShift route (the component providing the external connection) as part of your CI/CD pipeline to ensure it is correctly servicing web traffic external to your cluster2.</p>"},{"location":"on-premises-lab/application-promotion/promote/#create-external-route-test-task","title":"Create External Route Test Task","text":"<p>You will create a task to check the connection to your external route as part of your CI/CD pipeline.</p> <ol> <li> <p>Copy the <code>connection-test</code> task using the following definition (copy by clicking on the copy icon in the top right of the box below):</p> <p><pre><code>apiVersion: tekton.dev/v1beta1\nkind: Task\nmetadata:\nname: connection-test\nspec:\ndescription: &gt;-\n\"This task runs a bash script to determine if a given application\nis accessible to the outside world via its route.\"\nparams:\n- name: ROUTE_NAME\ndefault: \"\"\ndescription: \"The name of the OpenShift route for the application.\"\ntype: string\n- name: APP_PATH\ndefault: \"/\"\ndescription: \"The path to reach the application from it's hostname\"\ntype: string\n- name: EXPECTED_STATUS\ndefault: \"200\"\ndescription: \"The expected http(s) status code from querying the application.\"\ntype: string\n- name: TIMEOUT\ndefault: \"30\"\ndescription: \"The number of seconds to try before giving up on a successful connection.\"\ntype: string\n- name: SECURE_CONNECTION\ndefault: \"true\"\ndescription: \"true for a secure route (https), false for an insecure (http) route.\"\ntype: string\nsteps:\n- name: route-connection-test\nimage: 'image-registry.openshift-image-registry.svc:5000/openshift/cli:latest'\nresources:\nlimits:\ncpu: 200m\nmemory: 200Mi\nrequests:\ncpu: 200m\nmemory: 200Mi\nscript: |\n#!/usr/bin/env bash\n# Make parameters into variables for clarity\nexport route_name=\"$(params.ROUTE_NAME)\"\nexport expected_status=\"$(params.EXPECTED_STATUS)\"\nexport app_path=\"$(params.APP_PATH)\"\nexport timeout=\"$(params.TIMEOUT)\"\nexport secure_connection=\"$(params.SECURE_CONNECTION)\"\n\n# If true, http(s), if false (or otherwise) http\nif [ \"${secure_connection}\" == \"true\" ]\nthen\nexport header=\"https://\"\necho \"Using secure https connection...\"\nelse\nexport header=\"http://\"\necho \"Using insecure http connection...\"\nfi\n# Start timer at 0\nSECONDS=0\n# Once timeout reached, stop retrying\nwhile [ \"${SECONDS}\" -lt \"${timeout}\" ];\ndo\n# Get hostname of route\nhostname=\"$(oc get route ${route_name} -o jsonpath='{.spec.host}')\"\n# Get http(s) status of web page via external connection (route)\nstatus=$(curl -sk -o /dev/null -w \"%{http_code}\" \"${header}${hostname}${app_path}\")\n# Print test completion message if expected status code received\nif [ \"${status}\" -eq \"${expected_status}\" ]\nthen\necho \"---------------------------TESTS COMPLETE---------------------------\"\necho \"Congratulations on a successful test!\"\necho \"Please visit the application at:\"\necho\necho \"${header}${hostname}${app_path}\"\nexit 0\n# Print failure message if incorrect status code received + retry\nelse\necho \"The application is unexpectedly returning http(s) code ${status}...\"\necho \"It is not available to outside traffic yet...\"\necho \"Retrying in 5s at:\"\necho\necho \"${header}${hostname}${app_path}\"\nsleep 5\nfi\ndone\n# Redirect output to standard error, print message, and exit with error after timeout\n&gt;&amp;2 echo \"Error, failed after ${timeout} seconds of trying...\"\n&gt;&amp;2 echo \"The application was never accessible to the outside world :(\"\nexit 1\n</code></pre> 2. Create the <code>connection-test</code> Task</p> <ol> <li> <p>Click <code>Import YAML</code> to bring up the box where you can create Kubernetes resource definitions from yaml</p> </li> <li> <p>Paste the <code>connection-test</code> Task into the box</p> </li> <li> <p>Scroll down and click create to create the <code>connection-test</code> Task </p> </li> </ol> <p></p> </li> </ol> <p>You should now see the created <code>connection-test</code> Task. Finally, navigate back to the <code>Pipelines</code> section of the OpenShift UI and go back to editing your pipeline.</p> <p></p>"},{"location":"on-premises-lab/application-promotion/promote/#add-external-route-test-task-to-pipeline","title":"Add External Route Test Task to Pipeline","text":"<ol> <li> <p>Add a sequential task after <code>deploy-dev</code>. When you <code>Add Task</code>, choose the <code>connection-test</code> task. </p> <p></p> </li> <li> <p>Configure <code>connection-test</code> task</p> <p>The only values you need to change are the <code>Display Name</code> and the <code>ROUTE_NAME</code> (copy and paste boxes below image):</p> <p></p> Display Name<pre><code>connect-dev\n</code></pre> ROUTE_NAME<pre><code>spring-petclinic-dev\n</code></pre> </li> <li> <p><code>Save</code> the pipeline</p> </li> </ol> <p>Your current pipeline builds and tests your application, creates a docker image for it, deploys it to the development environment, and ensures that the application is working both internally and externally. In other words, once your application successfully completes the current pipeline, you can be confident in it and be ready to move to staging3. </p>"},{"location":"on-premises-lab/application-promotion/promote/#deploy-staging","title":"Deploy Staging","text":"<p>Moving to the staging environment means spinning up your application in that environment (with parameters relevant for it) and testing it there. Given that this is all using containers, you can easily free up the development resources that have successfully completed and then spin up the new resources in your staging environment.</p>"},{"location":"on-premises-lab/application-promotion/promote/#remove-dev","title":"Remove Dev","text":"<p>Your first Task will mirror the <code>cleanup-resources</code> task at the beginning of your pipeline, but will just cleanup the <code>dev</code> resources using the <code>env=dev</code> label selector.</p> <ol> <li> <p>Go back to editing your pipeline via <code>Actions -&gt; Edit Pipeline</code></p> <p></p> </li> <li> <p>Add a Task sequentially at the end of the pipeline (after <code>connect-dev</code>) using the <code>openshift-client</code> ClusterTask.  </p> <p></p> </li> <li> <p>Configure the Task with the following values (copy and paste boxes below image):</p> <p></p> Display Name<pre><code>cleanup-dev\n</code></pre> SCRIPT<pre><code>oc delete deployment,cm,svc,route -l app=spring-petclinic,env=dev --ignore-not-found\n</code></pre> </li> </ol>"},{"location":"on-premises-lab/application-promotion/promote/#add-staging","title":"Add Staging","text":"<p>You will use your existing <code>kustomize</code> task to deploy the staging configuration for your PetClinic application in a new <code>kustomize-staging</code> task. Customizations for staging PetClinic include adding a staging environment label, name suffix, change cause, and staging environment variables for your application. You could deploy to a separate project or cluster altogether as well as change replicas or add pod autoscalers in a similar manner (depending on your use case) for different environments. </p> <ol> <li> <p>Add a <code>kustomize</code> task sequentially to the end of your current pipeline (after <code>cleanup-dev</code>)</p> <p> </p> </li> <li> <p>Configure the Task with the following values (copy and paste boxes below image):</p> <p></p> Display Name<pre><code>kustomize-staging\n</code></pre> RELEASE_SUBDIR<pre><code>overlay/staging\n</code></pre> SCRIPT<pre><code>kustomize edit set image spring-petclinic=$(params.IMAGE_NAME)-minimal:$(params.COMMIT_SHA)\n</code></pre> SOURCE (choose from dropdown)<pre><code>workspace\n</code></pre> </li> </ol>"},{"location":"on-premises-lab/application-promotion/promote/#rollout-staging","title":"Rollout Staging","text":"<ol> <li> <p>Edit the pipeline again and add a <code>deploy-staging</code> task with the <code>openshift-client</code> ClusterTask</p> <p></p> </li> <li> <p>Configure the task with the following parameters4 (copy and paste boxes below image):</p> <p></p> Display Name<pre><code>deploy-staging\n</code></pre> SCRIPT<pre><code>echo \"$(params.GIT_MESSAGE)\" &amp;&amp; oc rollout status deploy/spring-petclinic-staging\n</code></pre> </li> </ol>"},{"location":"on-premises-lab/application-promotion/promote/#add-external-route-test-task-to-pipeline_1","title":"Add External Route Test Task to Pipeline","text":"<ol> <li> <p>Add a sequential task after <code>deploy-staging</code>. When you <code>Select Task</code>, choose the <code>connection-test</code> task. </p> <p></p> </li> <li> <p>Configure <code>connection-test</code> task with the following parameters (copy and paste boxes below image):</p> <p></p> Display Name<pre><code>connect-staging\n</code></pre> ROUTE_NAME<pre><code>spring-petclinic-staging\n</code></pre> </li> <li> <p><code>Save</code> the pipeline</p> <p></p> </li> </ol>"},{"location":"on-premises-lab/application-promotion/promote/#summary","title":"Summary","text":"<p>Congratulations! You have built a pipeline that tests your <code>PetClinic</code> application, creates a docker image for it, deploys it to the development environment with dev configuration, ensures that the application is working both internally and externally, cleans up the development environment, deploys it to the staging environment with staging configuration, and then makes sure it is working both internally and externally5. </p> <p>tl;dr</p> <p>You now have the <code>I/D</code> (Integration/Deployment) in <code>CI/CD</code>6.</p> <ol> <li> <p>For different environments like dev and test, this may be different groups external to your Kubernetes environment (cluster), though internal to the organization itself and accessing the endpoints via a VPN or internal network. Production is likely when external connection via an organization's real website would happen. The type of external connection (via a VPN or public connection) has little impact on the Kubernetes resources given a route will be used for all of those types of external connections (the most important thing is that the route you are testing is available to you [the tester] from where you are).\u00a0\u21a9</p> </li> <li> <p>You may think to yourself that you can't test an external connection from inside your cluster. However, by using the route, you are causing the traffic to go \"outside\" the cluster's networking to reach the load balancer and then back \"inside\" via the route. This explicitly tests the external connection and makes sure that it indeed works. However, you are just testing that the route works, not that the dns/hostname is available generally on the internet or private enterprise subnet (depending on environment). Internet / subnet dns resolution is a different, more general problem for your networking team (or cloud provider) to ensure for all of the applications using that network.\u00a0\u21a9</p> </li> <li> <p>You could create more extensive tests to make sure that the pages are rendering correctly (besides just returning a proper status code). However, that is beyond the scope of the lab and this at least makes sure requests are successfully sent and returned via an external route, which is good enough for the lab's purposes.\u00a0\u21a9</p> </li> <li> <p>This mirrors the <code>dev-deploy</code> task which waits for the dev release to rollout but uses the <code>SCRIPT</code> field for everything vs. <code>ARGS</code>.\u00a0\u21a9</p> </li> <li> <p>You could clean up the staging environment at the end of the run but choose not to so that the user can interact with it between runs. You could also clean up or use a separate MySQL instance for staging but due to limited resources in your environment you have chosen not to add this extra component.\u00a0\u21a9</p> </li> <li> <p>You'll add the double <code>C</code>s in the next section by connecting it to Gogs.\u00a0\u21a9</p> </li> </ol>"},{"location":"on-premises-lab/build-and-deploy/","title":"It's Time to Open up your Pet Clinic for Testing","text":"<p>In this section, you will build, test and deploy the Java web application for your pet clinic (PetClinic) to get it up and running on OpenShift. This involves the following tasks:</p> <ul> <li>Deploying MySQL database</li> <li>Building and deploying PetClinic with automated testing</li> <li>Accessing PetClinic and adding an owner</li> </ul>"},{"location":"on-premises-lab/build-and-deploy/upandrunning/","title":"Getting Your PetClinic Application Up and Running","text":"<p>For this workshop you will be using the iconic Spring PetClinic application. The Spring PetClinic is a sample application designed to show how the Spring stack can be used to build simple, but powerful database-oriented applications. The official version of PetClinic demonstrates the use of Spring Boot with Spring MVC and Spring Data JPA. </p> <p>You will not be focusing on the ins and outs of the PetClinic application itself, but rather on leveraging OpenShift tooling to build a PetClinic cloud native application and a DevSecOps pipeline for the application.</p> <p>You will start by building your PetClinic application from the source code and connecting it to a MySQL database.</p> <p>Lab Guide</p> <ul> <li>For the images in this lab:</li> <li>the green arrows or boxes denote something to look at or reference </li> <li>the red arrows or boxes denote something to click on or type.</li> </ul> <p>OpenShift Cluster</p> <p>You will be logging into our on-premises s390x architecture (IBM Z / IBM LinuxONE) OpenShift cluster for this lab. You will each be given a student user and your project name will be studentxx where xx is your student number. The project name depicted in the diagrams below is student00. You will be operating in your assigned project for the entirety of the lab.</p> <p>Eliminate Extra Newlines</p> <p>There are copy and paste boxes throughout the lab. When using these boxes, sometimes a newline will be added to the pasted result (you will know it when you see it as your cursor will be moved to the next line with whitespace shown). Please delete these newlines to stay on the happy path .</p>"},{"location":"on-premises-lab/build-and-deploy/upandrunning/#logging-into-the-on-premises-wsc-openshift-cluster","title":"Logging into the on-premises WSC OpenShift Cluster","text":"<ol> <li> <p>You will login with your assigned user and password using the link here.</p> </li> <li> <p>Navigate to the project of your assigned user (it will be studentxx where xx is your number for the lab):</p> </li> </ol> <p></p>"},{"location":"on-premises-lab/build-and-deploy/upandrunning/#deploying-mysql-database","title":"Deploying MySQL database","text":"<ol> <li> <p>First, you need to setup your mysql database. Luckily, this is very easy on OpenShift with the mysql template available from the main developer topology window. Follow the steps in the diagram below to bring up the available database options. (Note your project name will be different than the picture below)</p> <p></p> <p>Now you can start the lab!</p> </li> <li> <p>Next, select the <code>MySQL (Ephemeral)</code> tile. </p> <p>Why Ephemeral?</p> <p>You are using the Ephemeral implementation because this a short-lived demo and you do not need to retain the data.  In a staging or production environment, you will most likely be using a MySQL deployment backed by a Persistent Volume Claim. This stores the data in a Persistent Volume (basically a virtual hard drive), and the data will persist beyond the life of the container.</p> <p></p> </li> <li> <p>Click on instantiate template.</p> <p></p> </li> <li> <p>Fill the wizard with the parameters as shown in the image below (your namespace will be different from the image below):</p> <p></p> <pre><code>petclinic\n</code></pre> <p>Click the <code>Create</code> button. </p> </li> <li> <p>A minute or two later, in the <code>Topology</code> view of your OpenShift Console, you should see <code>mysql</code> in the <code>Running</code> state. (Click on the Topology icon for <code>mysql</code> to bring up the side panel)</p> <p></p> </li> </ol>"},{"location":"on-premises-lab/build-and-deploy/upandrunning/#find-the-repo-in-your-gogs-account","title":"Find the Repo in your gogs account","text":"<ol> <li> <p>Log into the locally hosted gogs git server here (if not already logged in) using your username and password.</p> </li> <li> <p>Navigate to the spring-petclinic repo by clicking on it on the right side of the login page.</p> <p></p> </li> <li> <p>Copy your repo URL using the copy button for the next section.</p> </li> </ol> <p></p> <p>That's it! You are ready to move on to the next section.</p>"},{"location":"on-premises-lab/build-and-deploy/upandrunning/#building-and-deploying-petclinic-application","title":"Building and Deploying PetClinic Application","text":"<p>There are multiple ways OpenShift enables cloud native application developers to package up their applications and deploy them. For PetClinic, you will be building your container image from source, leveraging OpenShift's S2I (Source to Image) capability. This allows you to quickly test the building, packaging, and deployment of your application, and gives you the option to create and use a DevSecOps pipeline from this workflow. It's a good way to start to understand how OpenShift Pipelines work.</p> <ol> <li> <p>Start with choosing Add From Git:</p> <p></p> </li> <li> <p>Enter the repo URL you copied before in the <code>Git Repo URL</code> field. Expand the <code>Show Advanced Git Options</code> section, and type in <code>main</code> for the <code>Git Reference</code>. This tells OpenShift which git repo and branch to pull the source code from.</p> <p></p> </li> <li> <p>Scroll down to the <code>Builder</code> section. Select the <code>Java</code> tile and select <code>openj9-11-el8</code> as the builder container image version. As you can see OpenShift offers many different builder container images to help you build container images from a variety of programming languages. Your list of builder container images might differ from the screen shot. For Java on Z, the recommended JVM is <code>OpenJ9</code> because it has built-in s390x optimizations as well as container optimizations.</p> <p></p> </li> <li> <p>In the General section, put in the following entries for Application Name and Name. </p> <p></p> </li> <li> <p>Scroll down to the  Pipelines section, select the checkbox next to <code>Add pipeline</code>. You can also expand the <code>Show pipeline visualization</code> section to see a visual of the build pipeline.</p> <p></p> </li> <li> <p>You are almost there! You will need to configure a couple of Advanced Options. First, click on <code>Show advanced Routing options</code> in the Advanced Options section to expand the Routing options.</p> <p></p> </li> <li> <p>In the Routing options section, only fill out the Security options as follows. You can leave the rest alone. These options will enable only TLS access to your PetClinic application.</p> <p></p> </li> <li> <p>You are done with configurations of this panel. Scroll all the way down and hit the <code>Create</code> button which will kick off the pipeline build of your PetClinic application. In a few seconds you will see your Topology with the new application icon. Hit the little pipeline icon in the diagram below to view the build logs.  You might see errors associated with ImageStream not being able to pull the application image during the build process. This does not mean that the build has failed. The pipeline creates the ImageStream first and then goes through the actual build process, and since the build process takes 10-15 minutes to complete, this error will be there until then. </p> <p></p> </li> <li> <p>The pipeline will go through three tasks:</p> <p>\u00a0\u00a0\u00a0 1. fetch-repository - this Pipeline task will git clone your PetClinic repo for the build task.</p> <p>\u00a0\u00a0\u00a0 2. build - this Pipeline task is the build process which itself is broken down into a few sub-steps. This is the longest task in the pipeline, and can take up to 15 minutes. The steps that it goes through are as follows:</p> <p>build steps</p> <ul> <li>STEP-GENERATE: this step generates the environment and Dockerfiles that will be used to create the OCI container image later on during the build and push step</li> <li>STEP-BUILD-AND_PUSH: this is the multi-step build process of creating an OCI container image out of your Java application PetClinic followed by a push. It will download the required Maven Java packages, compile the Java application, run through a set of 39 unit tests on the application, and finally build the application jar file and the OCI container image. If the tests fail, this step will not complete. Finally, this step pushes the built OCI container image to the OpenShift image registry.</li> </ul> <p>\u00a0\u00a0\u00a0 3. deploy - this Pipeline task will deploy the newly built container image as a running deployment in your project. After this, your application will be running in a pod and be accessible via a route.</p> <p>Below is an image of the log of a successful build task: </p> <p></p> </li> <li> <p>Now if you go back to the Topology view, you should see the application has been successfully deployed to OpenShift as well. From here you can click on the <code>open URL</code> circle, and a new browser tab should open to lead you to your PetClinic's front page.  It can take a couple of minutes before the application is accessible through its URL so if it doesn't come up right away wait a few minutes and try again. </p> <p></p> </li> </ol>"},{"location":"on-premises-lab/build-and-deploy/upandrunning/#interacting-with-your-petclinic-application-and-mysql-database","title":"Interacting with Your PetClinic Application and MySQL database","text":"<p>In this section, you will add a new owner to the Pet Clinic application, and then go into your MySQL container to see if that owner was successfully added. </p> <p>1. Your Pet Clinic should look something similar to this. Go to the Find Owners tab, and create a new owner.</p> <p></p> <p>2. Click on the <code>Add Owner</code> button, and add an owner of your own, for example:</p> <p></p> <p>3. You can then go back to <code>Find Owners</code> and try searching for the owner that you just added. It should come back with the search results similar to the following.</p> <p></p> <p>4. Now let's check the MySQL database to make sure that the new owner you just added is in there.</p> <p>Return to your OpenShift console, from the Topology view, click on the <code>mysql</code> icon. This will bring up a side panel, and then click on the <code>mysql</code> pod (your pod name will be different than the picture):</p> <p></p> <p>In the pod panel, go to the Terminal tab.</p> <p></p> <p>Now type in the following commands in your <code>mysql</code> terminal (copy and paste box below image):</p> <p></p> <pre><code>mysql -u root -h mysql -ppetclinic\n</code></pre> <pre><code>use petclinic;\n</code></pre> <pre><code>show tables;\n</code></pre> <p>Let's run a SQL command now to verify that the owner that you added through the application is indeed in the database (copy and paste box below image):</p> <p></p> <pre><code>select * from owners;\n</code></pre> <p>Tip</p> <p>If you added a different user than alice you should see that user in place of alice on your screen.</p> <p>Please let the instructors know, if you don't see your owner you added listed. </p>"},{"location":"on-premises-lab/build-and-deploy/upandrunning/#summary","title":"Summary","text":"<p>Congratulations, you have completed this part of the workshop! You have your virtual pet clinic up and running and have created an OpenShift Pipelines pipeline that you will build on in the next sections of the lab to achieve CI/CD. You may move on to the next part by clicking <code>Next</code> on the bottom right of the page.</p>"},{"location":"on-premises-lab/devsecops/devsecops/","title":"Configure SonarQube code analysis in your Pipeline","text":"<p>As a bonus lab, you will now configure an extra task in your existing Pipeline to conduct code scanning on your petclinic source code. This exercise is to show you one way of incorporating security code scanning as part of your automated CI/CD pipeline.</p> <p>We will use the popular open source package SonarQube to do the code scanning. According to Wikipedia, \"SonarQube is an open-source platform developed by SonarSource for continuous inspection of code quality to perform automatic reviews with static analysis of code to detect bugs, code smells, and security vulnerabilities on 20+ programming languages.\"</p> <p>For Petclinic, we will be using SonarScanner for Maven. The ability to execute the SonarQube analysis via a regular Maven goal makes it available anywhere Maven is available (developer build, CI server, etc.), without the need to manually download, setup, and maintain a SonarQube Runner installation. For more information on SonarScanner for Maven, please see here.</p>"},{"location":"on-premises-lab/devsecops/devsecops/#configuring-maven-settings-with-the-sonar-scanner-plugin","title":"Configuring maven-settings with the Sonar scanner plugin","text":"<p>We need to configure maven with the Sonar scanner plugin prefix. We will do that by including the sonar scanner plugin in the maven settings file.</p> <p>We will create a Kubernetes ConfigMap for the mavens settings file.</p> <p>Click on the Import Yaml button at the top of the OpenShift console (the '+' symbol).</p> <p>Copy and paste the entirety of the following into the editor and then hit \"Create\" (copy by clicking on the copy icon in the top right of the box below).</p> <pre><code>kind: ConfigMap\napiVersion: v1\nmetadata:\n  name: maven-settings\ndata:\n  settings.xml: |\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n    &lt;settings&gt;\n      &lt;pluginGroups&gt;\n        &lt;pluginGroup&gt;io.spring.javaformat&lt;/pluginGroup&gt;\n        &lt;pluginGroup&gt;org.sonarsource.scanner.maven&lt;/pluginGroup&gt;\n      &lt;/pluginGroups&gt;\n      &lt;profiles&gt;\n        &lt;profile&gt;\n          &lt;id&gt;sonar&lt;/id&gt;\n          &lt;activation&gt;\n            &lt;activeByDefault&gt;true&lt;/activeByDefault&gt;\n          &lt;/activation&gt;\n          &lt;properties&gt;\n            &lt;!-- Wait until the quality check is complete in SonarQube  --&gt;\n            &lt;sonar.qualitygate.wait&gt;\n              true\n&lt;/sonar.qualitygate.wait&gt;\n            &lt;!-- Exclude DTO Files from SonarQube duplication check as these should have duplications --&gt;\n            &lt;sonar.cpd.exclusions&gt;\n              **/*DTO*\n            &lt;/sonar.cpd.exclusions&gt;\n          &lt;/properties&gt;\n        &lt;/profile&gt;\n      &lt;/profiles&gt;\n    &lt;/settings&gt;\n</code></pre>"},{"location":"on-premises-lab/devsecops/devsecops/#accessing-the-sonarqube-server-with-your-assigned-credentials","title":"Accessing the SonarQube server with your assigned credentials","text":"<p>The lab instructors have already setup a SonarQube server within the OpenShift cluster for you to access for code scanning. Credentials have also been setup for you. Please use your assigned credentials to test access to the SonarQube Server.</p> <ol> <li> <p>Access the SonarQube server here</p> </li> <li> <p>Select <code>Log in</code> in the upper right hand corner. And log in with your assigned credentials.</p> <p>If you are not successful with this step</p> <p>Please let an instructor know.</p> </li> </ol>"},{"location":"on-premises-lab/devsecops/devsecops/#generate-a-security-token-for-your-sonarqube-account","title":"Generate a security token for your SonarQube account","text":"<p>You'll need either your credentials, or an access token associated with your account, in order to access the server for code scanning. </p> <p>Let's use the access token method.</p> <p>Now that you've logged in, select your account in the upper right hand corner of the SonarQube server page.</p> <p> </p> <p>In the account panel, go to the security tab, and type in the name <code>petclinic</code> to help identify your token, and then select <code>Generate</code>. Now copy and save this token or leave the page/tab open to copy it when you need it in the next subsection.</p> <p> </p>"},{"location":"on-premises-lab/devsecops/devsecops/#configuring-maven-task-into-pipeline-to-do-code-analysis","title":"Configuring maven task into Pipeline to do code analysis","text":"<p>Go back to your OpenShift console and go to your pipeline. Your pipeline should look like the picture below, at this point of the workshop.</p> <p></p> <ol> <li> <p>Add <code>maven-settings</code> workspace to your pipeline</p> <p></p> <ol> <li> <p>Click <code>Add workspace</code></p> </li> <li> <p>Name the workspace</p> <pre><code>maven-settings\n</code></pre> </li> <li> <p>Click <code>Save</code></p> </li> </ol> </li> <li> <p>We will insert the code analysis task before the build task. The idea being we want to scan the source code for bugs and vulnerabilities, before we build a container image out of it.</p> <p>a. From your pipeline screen, Go to Actions -&gt; Edit Pipeline.</p> <p>b. Select the plus sign before the build task, as in the picture below.</p> <p></p> <p>c. Then search for and select the <code>maven</code> task.</p> <p></p> <p>Tip</p> <p>Once you add a specific task (i.e. <code>maven</code>), clicking on the oval of the task will enable you to edit its default values for your needs.</p> </li> <li> <p>Give the task the following parameters to do the code analysis with the proper maven goals set to do code scanning against our SonarQube server.</p> <p></p> Display Name<pre><code>code-analysis\n</code></pre> MAVEN_IMAGE<pre><code>maven:3.8.1-jdk-11-openj9\n</code></pre> <p>GOALS</p> GOAL 1<pre><code>package\n</code></pre> GOAL 2<pre><code>sonar:sonar\n</code></pre> GOAL 3<pre><code>-Dsonar.login=&lt;use-your-token-from-previous-step&gt;\n</code></pre> <p>Use your token</p> <p>You need to replace <code>&lt;use-your-token-from-previous-step&gt;</code> with your actual token.</p> GOAL 4<pre><code>-Dsonar.host.url=http://sonarqube.sonarqube:9000\n</code></pre> GOAL 5<pre><code>-Dsonar.projectName=petclinic-&lt;your-student&gt;\n</code></pre> <p>Use your student...</p> <p>Be mindful to put your student in the value of the <code>Dsonar.projectName</code> and <code>`Dsonar.projectKey</code> goals (i.e., substitute <code>&lt;your-student&gt;</code> with your student such as <code>petclinic-student00</code>).</p> GOAL 6<pre><code>-Dsonar.projectKey=petclinic-&lt;your-student&gt;\n</code></pre> <p>...Please use your student</p> <p>Remember to replace <code>&lt;your-student&gt;</code> with your student such as <code>petclinic-student00</code>.</p> SOURCE (choose from dropdown)<pre><code>workspace\n</code></pre> MAVEN-SETTINGS (choose from dropdown)<pre><code>maven-settings\n</code></pre> </li> <li> <p>Now you can click away to get back to the main pipeline edit panel.</p> </li> <li> <p>Save the <code>pipeline</code>.</p> </li> </ol>"},{"location":"on-premises-lab/devsecops/devsecops/#add-the-new-maven-settings-workspace-to-the-triggertemplate","title":"Add the new <code>maven-settings</code> workspace to the TriggerTemplate","text":"<ol> <li> <p>Go to the TriggerTemplates section of your pipeline and click the link to take you to your pipeline's <code>TriggerTemplate</code></p> <p></p> </li> <li> <p>Edit the <code>TriggerTemplate</code></p> <ol> <li>Click Actions</li> <li>Choose <code>Edit TriggerTemplate</code> from the dropdown menu</li> </ol> <p></p> </li> <li> <p>Add the workspace to the <code>workspaces</code> section of the TriggerTemplate.</p> <ol> <li> <p>Add the following code to the <code>workspaces</code> section</p> <pre><code>          - name: maven-settings\n            configMap:\n              name: maven-settings\n</code></pre> <p>Indentation Matters!</p> <p>Take care to match the indentation in the picture below</p> </li> <li> <p>Click <code>Save</code> to apply your changes</p> </li> </ol> <p></p> </li> </ol>"},{"location":"on-premises-lab/devsecops/devsecops/#run-the-pipeline","title":"Run the pipeline","text":"<p>Go to the Actions menu of your pipeline and select Start.</p> <p></p> <p>Hit Start after reviewing the settings panel and making sure to set the options for the <code>source</code>(select <code>PersistentVolumeClaim</code> and your claim) and <code>maven-settings</code> ( select <code>configmap</code> as the resource choice and <code>maven-settings</code> as the specific configmap to use as in the image below) workspaces.</p> <p></p> <p>You can go to your pipeline logs and see the output for each of the tasks.  <p>It will take 15-20 minutes for the code analysis to run completely through. This task will wait until the quality check is complete in SonarQube and if the quality gate fails, this task will fail and the pipeline will not continue to run. If the quality gate succeeds, this task will succeed and progress onto the next task in the pipeline.</p> <p>Let's see if our code passes the code analysis...</p> <p></p> <p>It fails . Next, we are going to see why it failed.</p>"},{"location":"on-premises-lab/devsecops/devsecops/#analyzing-the-failure-in-sonarqube","title":"Analyzing the Failure in SonarQube","text":""},{"location":"on-premises-lab/devsecops/devsecops/#view-your-project","title":"View your project","text":"<p>At this point please return to the SonarQube server here, and view the code scan report to see what caused the quality check to fail. After logging in, please do the following:</p> <p></p> <ol> <li> <p>Type your student in the project search bar to bring up your project</p> </li> <li> <p>Click on your project (which should have a <code>Failed</code> label)</p> </li> </ol>"},{"location":"on-premises-lab/devsecops/devsecops/#check-what-caused-the-failure","title":"Check what caused the failure","text":"<p>You can see that the overall code check failed due to a security rating worse than A. You should see 9 vulnerabilities that caused this failure. In order to check what these are, please click on the vulnerabilities link as shown in the image.</p> <p></p> <ol> <li> <p>See individual vulnerabilities and click on <code>Why is this an issue?</code></p> </li> <li> <p>Read the vulnerability descriptions to see why they are a problem and get insights into fixing them in the code.</p> </li> </ol>"},{"location":"on-premises-lab/devsecops/devsecops/#update-petclinic-to-fix-the-issues-that-came-up-in-the-sonarqube-scan","title":"Update PetClinic to fix the issues that came up in the SonarQube scan","text":"<p>In the scan, there were various security issues related to the use of entity objects for data transfer instead of data transfer objects (DTOs) when using @RequestMapping and similar methods. In order to fix these, you will have to make changes to the java code for the application. Luckily for you, the changes have already been made on the <code>security-fixes</code> branch of the project. In order to bring these changes to the main branch you will need to make a pul request and merge the <code>security-fixes</code> branch into the main branch. </p> <p>You can do this with the following actions:</p> <ol> <li> <p>Go to your petclinic repository in Gogs and create a new pull request</p> <p></p> <ol> <li> <p>Navigate to the <code>Files</code> tab of your repository</p> </li> <li> <p>Click on the green pull request button to the left of the branch</p> </li> </ol> </li> <li> <p>Choose the <code>security-fixes</code> branch to merge into the <code>main</code> branch</p> <p></p> </li> <li> <p>Write a justification for your pull request and create it</p> <p></p> <ol> <li> <p>Give your pull request a title such as:</p> <pre><code>Security Fixes\n</code></pre> </li> <li> <p>Write a justification such as </p> <pre><code>Create fixes for all of the security vulnerabilities that showed up in the SonarQube scan.\n</code></pre> </li> <li> <p>Click <code>Create Pull Request</code></p> </li> </ol> </li> <li> <p>Merge your pull request, merging the <code>security-fixes</code> branch with all of the security fixes into the <code>main</code> branch.</p> <p></p> </li> <li> <p>Delete the <code>security-fixes</code> branch now that it's been successfully merged into the <code>main</code> branch of your petclinic repository fork.</p> <p></p> <ol> <li> <p>See that the <code>security-fixes</code> branch was successfully merged!</p> </li> <li> <p>Click <code>Delete branch</code> to delete the now superfluous <code>security-fixes</code> branch.</p> </li> </ol> </li> </ol>"},{"location":"on-premises-lab/devsecops/devsecops/#verify-that-vulnerabilities-in-petclinic-have-been-patched","title":"Verify that vulnerabilities in petclinic have been patched","text":"<ol> <li> <p>See a new pipeline triggered back in the <code>Pipelines</code> view of your OpenShift namespace.</p> <p></p> </li> <li> <p>View the pipeline run and watch it successfully complete the <code>code-analysis</code> task.</p> <p></p> <p>Note</p> <p>You can also wait to see the other tasks pass but since the main goal of this section was to focus on integrating security into DevOps and you have already gone through the pipeline without the <code>code-analysis</code> task, there is really no need to do so.</p> </li> <li> <p>View the SonarQube server again to see the updated results for your project (based on the latest scan)</p> <ol> <li> <p>See your project passes and click on it for full results</p> <p>Tip</p> <p>Search for your project with your student like before.</p> <p></p> </li> <li> <p>View the final results of the scan.</p> <p></p> <p>Those pesky vulnerabilities have been squashed! </p> </li> </ol> </li> </ol>"},{"location":"on-premises-lab/devsecops/devsecops/#summary","title":"Summary","text":"<p>In this section, you started on your DevSecOps journey by integrating SonarQube security scanning into your DevOps pipeline. Initially, the scan flagged several security vulnerabilities, causing the pipeline to fail before the vulnerable code could get packaged into a container. Next, you were able to dig into the vulnerabilities and figure out what needed to be changed with the SonarQube report. Then, you applied a security patch, eliminating the flagged security vulnerabilities in the PetClinic application. With these changes, your pipeline succeeded having containerized and deployed secure code. Finally, you are left with a pipeline set up to catch any new security vulnerabilities as soon as they appear. Congratulations!</p>"},{"location":"on-premises-lab/full-dev-pipeline/","title":"Configure PetClinic's Integration and Deployment Pipeline to Meet Your Organization's Needs1","text":"<p>It's time to expand your pipeline to automate the integration and deployment process for your application (with specific configuration for your organization) using OpenShift Pipelines. This involves the following tasks:</p> <ol> <li> <p>Automate PetClinic Build and Test for your Organization's Needs</p> <ul> <li>Automate MySQL deployment using OpenShift template</li> <li>Make clean container image from S2I build to meet the security needs of your organization</li> </ul> </li> <li> <p>Automate PetClinic development deployment to meet your Organization's Needs1</p> <ul> <li>Manage PetClinic deployment resources using KUSTOMIZE</li> <li>Setup PetClinic container image deployment automation with tagging based on source</li> </ul> </li> </ol> <ol> <li> <p>For the purposes of this lab, you are fulfilling the requirements of a fictional organization. These requirements could change for your specific organization but would follow a similar pattern with different specifics.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"on-premises-lab/full-dev-pipeline/pipeline/","title":"Configure PetClinic Build and Test to Meet your Organization's Requirements1","text":"<p>Now that PetClinic is up and running on your OpenShift cluster, it's time to add functionality to your pipeline to achieve basic integration and deployment when triggered. The OpenShift pipeline you created in the PetClinic Up and Running uses Tekton to run a series of tasks (each with one or more steps) to accomplish a workflow (pipeline). You will use the Pipeline Builder UI built into OpenShift to quickly and easily craft a pipeline that meets your specific needs.</p> <p>Why OpenShift Pipelines?</p> <ul> <li> <p>Portable: OpenShift resources defined via yaml files -&gt; portable across OpenShift clusters</p> </li> <li> <p>Low Resource Usage: Containers spin up when triggered -&gt; resources only used when needed</p> </li> <li> <p>Configurable: Can tailor overall pipeline and individual tasks to needs of your enterprise/organization </p> </li> <li> <p>Ease of Use: Pipeline Builder UI and built-in cluster resources (i.e. <code>ClusterTasks</code>, <code>ClusterTriggerBindings</code>, etc.) enable you to easily create a pipeline and export the yaml files with minimal knowledge.</p> </li> </ul>"},{"location":"on-premises-lab/full-dev-pipeline/pipeline/#petclinic-pipeline","title":"PetClinic Pipeline","text":"<p>When you deployed the PetClinic application using the <code>From Git</code> option in the PetClinic Up and Running section, you chose to create a basic pipeline. You'll start with this pipeline and edit it to add new functionality for your use case. Start editing by doing the following:</p> <ol> <li> <p>Navigate to the <code>Pipelines</code> tab in the <code>Developer</code> perspective on the left and then click on the <code>spring-petclinic</code> pipeline.</p> <p></p> </li> <li> <p>Click on the Actions dropdown on the far right and select <code>Edit Pipeline</code> </p> <p> </p> </li> </ol>"},{"location":"on-premises-lab/full-dev-pipeline/pipeline/#ensure-mysql-database-deployed-for-each-run","title":"Ensure MySQL Database Deployed for each Run","text":"<p>This will bring you to the Pipeline Builder UI where you can edit your pipeline. Here you will make sure the MySQL database is configured according to your specification before the <code>build</code> task.</p> <ol> <li> <p>Add a <code>mysql-rollout-wait</code> task in parallel to the <code>git-fetch</code> task.       </p> <p>Why is <code>mysql-rollout-wait</code> in Parallel?</p> <p>This ensures MySQL is in place for each <code>PetClinic</code> application build (which would fail without it).  </p> <p>Click <code>Add Task</code> in the middle of the rectangle of the new task and search <code>openshift-client</code> and choose it from the menu clicking on <code>Add</code> to choose it. </p> <p></p> <p>Click on the middle of the oval of the <code>openshift-client</code> task to enter values for it (copy and paste boxes below image).</p> <p></p> <p>Tip</p> <p>Once you add a specific task (i.e. <code>openshift-client</code>), clicking on the oval of the task will enable you to edit its default values for your needs.</p> <p>Give the task the following parameters to ensure the MySQL database is fully rolled out:</p> <p> Display Name<pre><code>mysql-rollout-wait\n</code></pre></p> SCRIPT<pre><code>oc rollout status dc/mysql\n</code></pre> <p>Simply Click Away</p> <p>Once you have entered the string into the <code>SCRIPT</code> section and deleted the help arg, just click away (i.e. on a regular section of the page) to get the configuration menu to go away and keep the new value(s) you just entered for the task.</p> </li> </ol> <p> Now your <code>mysql-rollout</code> task will make sure <code>MySQL</code> is rolled out for the <code>build</code> task!</p>"},{"location":"on-premises-lab/full-dev-pipeline/pipeline/#make-clean-image-from-s2i-build","title":"Make Clean Image from S2I build","text":"<p>The <code>s2i-java-11</code> container image is very convenient for making a container image from source code. However, the simplicity that gives it value can make it fail at meeting the needs of many organizations by itself. In your case, you will take the artifacts from the s2i container image and copy them to a new container image that can meet all your needs to get the best of both worlds. You'll create an optimized container image starting from a compact <code>openj9</code> java 11 base and employing the advanced layers feature in spring that optimizes Docker image caching with the final-Dockerfile in the ibm-wsc/spring-petclinic git repository you forked. </p> <ol> <li> <p>Add <code>Buildah</code> task</p> <p>Add the <code>buildah</code> task as a sequential task after the <code>build</code> task.</p> <p></p> <p>Choose Red Hat tasks</p> <p>There are various points throughout the lab (such as this one) where you have a choice when adding a task to select either the <code>Red Hat</code> or <code>Community</code> version of a Task. Please choose the <code>Red Hat</code> version in these instances.</p> <p></p> </li> <li> <p>Configure <code>buildah</code> task</p> <p>Tip</p> <p>Each value that you need to configure is listed below with the value in a click-to-copy window (other values can be left alone to match the image)</p> <p></p> Display Name<pre><code>clean-image\n</code></pre> IMAGE<pre><code>$(params.IMAGE_NAME)-minimal:$(params.COMMIT_SHA)\n</code></pre> DOCKERFILE<pre><code>./final-Dockerfile\n</code></pre> TLSVERIFY<pre><code>false\n</code></pre> BUILD_EXTRA_ARGS<pre><code>--build-arg PETCLINIC_S2I_IMAGE=$(params.IMAGE_NAME)\n</code></pre> SOURCE (choose from dropdown)<pre><code>workspace\n</code></pre> </li> <li> <p>Add <code>GIT_MESSAGE</code>, and <code>COMMIT_SHA</code> parameters to the pipeline</p> <p>Click <code>Add Parameter</code> twice ...</p> <p></p> <p>and then fill in the parameter details for <code>GIT_MESSAGE</code> and <code>COMMIT_SHA</code> (copy and paste boxes below image)</p> <p></p> <p>GIT_MESSAGE</p> Parameter Name<pre><code>GIT_MESSAGE\n</code></pre> Parameter Description<pre><code>Git commit message if triggered by Git, otherwise it's a manual build\n</code></pre> Parameter Default Value<pre><code>This is a manual build (not triggered by Git)\n</code></pre> <p>COMMIT_SHA</p> Parameter Name<pre><code>COMMIT_SHA\n</code></pre> Parameter Description<pre><code>SHA of Git commit if triggered by Git, otherwise just update manual tag\n</code></pre> Parameter Default Value<pre><code>manual\n</code></pre> <p>Tip</p> <p>Save the parameters when you are done with entry by clicking on blue <code>Save</code> box before continuing. If a blue <code>Save</code> box doesn't appear (is greyed out) delete extra blank parameters you may have accidentally added with the <code>-</code>.</p> </li> </ol>"},{"location":"on-premises-lab/full-dev-pipeline/pipeline/#summary","title":"Summary","text":"<p>Your pipeline will now automatically check that your <code>MySQL</code> instance is rolled out before moving on to the build stage (instead of doing this as a manual task like in the previous section of the lab). Moreover, it will curate the final PetClinic (<code>minimal</code>) container image to only have the necessary components instead of a bunch of extra packages (required only for the build itself) that add bloat and potential security vulnerabilities to your container image. Finally, it will tag the container image to distinguish between manual builds and those triggered by a potential git push. In the next section, you will see this automation in action for your development environment.</p> <ol> <li> <p>For the purposes of this lab, you are fulfilling the requirements of a fictional organization. These requirements could change for your specific organization but would follow a similar pattern with different specifics.\u00a0\u21a9</p> </li> </ol>"},{"location":"on-premises-lab/full-dev-pipeline/runpipeline/","title":"Configure PetClinic Development Deployment to Meet your Organization's Requirements1","text":""},{"location":"on-premises-lab/full-dev-pipeline/runpipeline/#manage-resource-across-environments-with-kustomize","title":"Manage resource across environments with Kustomize","text":"<p>Kustomize is a tool for customizing Kubernetes resource configuration.</p> <p>From the documentation overview</p> <p>Kustomize traverses a Kubernetes manifest to add, remove or update configuration options without forking. It is available both as a standalone binary and as a native feature of kubectl. See the Introducing Kustomize Kubernetes Blog Post for a more in-depth overview of Kustomize and its purpose.</p> <p>As part of doing things the \"cloud native\" way you will be using Kustomize to manage resource changes across your <code>dev</code> and <code>staging</code> environments as well as injecting information from your pipeline (such as newly created container image information with git commits) into your Kubernetes (OpenShift) resources. </p> <p>To see how you use Kustomize, see the Kustomize configuration in your <code>spring-petclinic</code> code in the subdirectories of the ocp-files directory.</p> <p>For more information on how kubectl (and oc through kubectl) integrates Kustomize, see the kubectl documentation.</p>"},{"location":"on-premises-lab/full-dev-pipeline/runpipeline/#creating-custom-task-for-kustomize","title":"Creating Custom Task for Kustomize","text":"<p>Since there is no ClusterTask defined for Kustomize, you will create a custom task for this purpose. It will change into the Kustomize directory, run a Kustomize command on the directory, and then apply the files from the directory using the built-in Kustomize functionality of the oc command line tool (via kubectl's Kustomize support)</p> <ol> <li> <p>Copy the <code>kustomize</code> task using the following definition (copy by clicking on the copy icon in the top right of the box below):</p> <pre><code>apiVersion: tekton.dev/v1beta1\nkind: Task\nmetadata:\nname: kustomize\nspec:\ndescription: &gt;-\nThis task runs commands against the cluster where the task run is being\nexecuted.\n\nKustomize is a tool for Kubernetes native configuration management.  It\nintroduces a template-free way to customize application configuration that\nsimplifies the use of off-the-shelf applications.  Now, built into kubectl\nas apply -k and oc as oc apply -k.\nparams:\n- default: ocp-files\ndescription: The directory where the kustomization yaml file(s) reside in the git directory\nname: KUSTOMIZE_DIR\ntype: string\n- default: base\ndescription: subdirectory of KUSTOMIZE_DIR used for extra configuration of current resources\nname: EDIT_SUDBDIR\ntype: string\n- default: overlay/dev\ndescription: subdirectory of KUSTOMIZE_DIR used for specifying resources for a specific release such as dev or staging\nname: RELEASE_SUBDIR\ntype: string\n- default: kustomize --help\ndescription: The Kustomize CLI command to run\nname: SCRIPT\ntype: string\nsteps:\n- image: 'quay.io/gmoney23/kustomize-s390x:v4.1.2'\nname: kustomize\nresources:\nlimits:\ncpu: 200m\nmemory: 200Mi\nrequests:\ncpu: 200m\nmemory: 200Mi\nworkingDir: \"$(workspaces.source.path)/$(params.KUSTOMIZE_DIR)/$(params.EDIT_SUDBDIR)\"\nscript: $(params.SCRIPT)\n- image: 'image-registry.openshift-image-registry.svc:5000/openshift/cli:latest'\nname: apply-oc-files\nresources:\nlimits:\ncpu: 200m\nmemory: 200Mi\nrequests:\ncpu: 200m\nmemory: 200Mi\nscript: oc apply -k \"$(workspaces.source.path)/$(params.KUSTOMIZE_DIR)/$(params.RELEASE_SUBDIR)\"\nworkspaces:\n- name: source\ndescription: The git source code\n</code></pre> </li> <li> <p>Create the <code>kustomize</code> Task</p> <p>a. Click <code>Import YAML</code> to bring up the box where you can create Kubernetes resource definitions from yaml</p> <p>b. Paste the <code>kustomize</code> Task into the box</p> <p>c. Scroll down and click <code>Create</code> to create the <code>kustomize</code> Task </p> <p></p> </li> </ol> <p>You should now see the created <code>kustomize</code> Task.</p> <p></p> <p>Finally, navigate back to the <code>Pipelines</code> section of the OpenShift UI and go back to editing your pipeline.</p>"},{"location":"on-premises-lab/full-dev-pipeline/runpipeline/#add-kustomize-task-to-pipeline","title":"Add Kustomize Task to Pipeline","text":"<ol> <li> <p>Add a sequential task after <code>clean-image</code> and when you <code>Select Task</code> choose the <code>kustomize</code> task. </p> <p></p> </li> <li> <p>Configure <code>kustomize</code> task</p> <p>Since your initial deploy will be for the <code>dev</code> environment, the only values you need to change are the <code>Display Name</code> and the <code>SCRIPT</code> (copy and paste boxes below image):</p> <p></p> Display Name<pre><code>kustomize-dev\n</code></pre> SCRIPT<pre><code>kustomize edit set image spring-petclinic=$(params.IMAGE_NAME)-minimal:$(params.COMMIT_SHA)\n</code></pre> SOURCE (choose from dropdown)<pre><code>workspace\n</code></pre> </li> <li> <p><code>Save</code> the pipeline</p> </li> </ol>"},{"location":"on-premises-lab/full-dev-pipeline/runpipeline/#clean-old-petclinic-instances-at-the-beginning-of-a-run","title":"Clean Old PetClinic Instances at the Beginning of a Run","text":"<ol> <li> <p>Go back to editing your pipeline via <code>Actions -&gt; Edit Pipeline</code></p> <p></p> </li> <li> <p>Add a Task named <code>cleanup-resources</code> sequentially at the beginning of the pipeline before <code>fetch-repository</code> (using the <code>openshift-client</code> ClusterTask).</p> <p></p> </li> <li> <p>Configure the task with the following parameters (copy and paste boxes below image for changes):</p> <p></p> Display Name<pre><code>cleanup-resources\n</code></pre> SCRIPT<pre><code>oc delete deployment,cm,svc,route -l app=$(params.APP_NAME) --ignore-not-found\n</code></pre> </li> </ol>"},{"location":"on-premises-lab/full-dev-pipeline/runpipeline/#update-deploy-task-to-deploy-dev","title":"Update Deploy Task to deploy-dev","text":"<ol> <li> <p>Click on the <code>deploy</code> Task at the end of the pipeline and change the following parameters to the corresponding values (copy and paste boxes below image):</p> <p></p> Display Name<pre><code>deploy-dev\n</code></pre> SCRIPT<pre><code>echo \"$(params.GIT_MESSAGE)\" &amp;&amp; oc rollout status deploy/spring-petclinic-dev\n</code></pre> </li> <li> <p><code>Save</code> your pipeline!</p> </li> </ol>"},{"location":"on-premises-lab/full-dev-pipeline/runpipeline/#run-the-updated-pipeline","title":"Run the Updated Pipeline","text":"<ol> <li> <p>Go to <code>Actions</code> -&gt; <code>Start</code> in the right hand corner of the pipeline menu</p> <p></p> </li> <li> <p>Manually trigger a <code>PipelineRun</code> by accepting the default values and clicking on <code>Start</code>.</p> <p>Persistent Volume Claim Note</p> <p>Please select a <code>PersistentVolumeClaim</code> if it is not already filled out for you to complete your pipeline. If it is already filled out for you then jump right to starting the pipeline.</p> <p></p> </li> <li> <p>Watch the results of your build pipeline run. It should complete successfully as in the pictures below.</p> <p>How long will your pipeline take to run?</p> <p>The pipeline run may take anywhere from 10-25 minutes to complete depending on the current system load. You can see the progress of your build, as well as if any errors occur, via the UI. Thus, by monitoring the UI, you can make sure things are going as planned.</p> <p>Pipeline Run Success View Perspective:</p> <p></p> <p>Pipeline Run Details View</p> <p>In the pipeline run <code>Details</code> view, you can see the pipeline run succeeded with all tasks having a green check mark. Additionally, the pipeline run in the above screenshot was <code>Triggered By</code> a user versus an automated source such as an event listener watching for a Gogs push...</p> <p>Pipeline Run Success Logs Perspective:</p> <p></p> <p>Pipeline Run Logs View</p> <p>From the pipeline run <code>Logs</code> view you can see that the pipeline run tasks all have green check marks and that this was a manual build (from the message in the log output of the final [<code>deploy-dev</code>] task).</p> </li> </ol>"},{"location":"on-premises-lab/full-dev-pipeline/runpipeline/#summary","title":"Summary","text":"<p>Congratulations! You successfully deployed your PetClinic application to your development environment with automated checks and configuration to meet your needs. This means that whenever your pipeline is triggered it will automatically spin up resources to build, test and deploy your application according to the specification you need to meet for your organization1.</p> <ol> <li> <p>For the purposes of this lab, you are fulfilling the requirements of a fictional organization. These requirements could change for your specific organization but would follow a similar pattern with different specifics.\u00a0\u21a9\u21a9</p> </li> </ol>"}]}